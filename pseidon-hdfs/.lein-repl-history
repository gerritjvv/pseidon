(use 'pseidon.hdsf.cloudera.core.datasource :reload)
(use 'pseidon.hdfs.cloudera.core.datasource :reload)
(defn coll! [coll f]
  (map #(delay (f %) %) coll))
(coll! [1 2 3 4] #(prn "hi " %))
(doc map)
(import 'org.apache.commons.lang.StringUtils)
(StringUtils/join "/" ["a", "b"])
(StringUtils/join ["a", "b"] "/")
(StringUtils/join ["a", "/b"] "/")
(use 'pseidon.test.hdfs.cloudera.core :reload)
(use 'pseidon.test.hdfs.cloudera.core)
(import org.apache.hadoop.fs.FileUtil)
(doc FileUtil/copy
)
(javadoc FileUtil/copy
)
(use 'pseidon.hdfs.cloudera.core.hdfs-processor)
(use 'pseidon.hdfs.cloudera.core.hdfs-processor :reload)
(use 'pseidon.hdfs.cloudera.core.hdfs-processor)
(use 'pseidon.hdfs.cloudera.core.hdfsprocessor)
(use 'pseidon.hdfs.cloudera.core.hdfsprocessor :reload)
(defn copy! []
           (go
             (while true
               (let [[ds local-file remote-file]  (<! c)]
                 ;retry till the file has been uploaded
                 (while (false? (file->hdfs ds local-file remote-file))
                   (Thread/sleep 1000))))))
    
    (defn copy! []
           (go
             (while true
               (let [[ds local-file remote-file]  (<! c)]
                 ;retry till the file has been uploaded
                 (while (false? (file->hdfs ds local-file remote-file))
                   (Thread/sleep 1000)))))
)
(defn ^:dynamic load-processor []
  (letfn [
    (parse-id [id] (StringUtils/split id ":") )
    
    (copy! []
           (go
             (while true
               (let [[ds local-file remote-file]  (<! c)]
                 ;retry till the file has been uploaded
                 (while (false? (file->hdfs ds local-file remote-file))
                   (Thread/sleep 1000))))))             
                   
    (start [])
    (stop [])
    (exec [ {:keys [topic ts ds] :as msg } ] ;unpack message
          (let [date-ts (from-long ts)
                dt (unparse dt-formatter date-ts) ;parse dt
                hr (unparse hr-formatter date-ts) ;parse hr
                ids (get-ids msg)  ;get the message ids as a sequence
                hdfs-dir (get-conf2 "hdsf-base-dir" "/log/raw") ;get the base hdfs dir
                ]
               (doseq [[internal-topic local-file] (map parse-id ids)] ;for each id (the id should be the local file-name) upload the file
                 (let [file-name (-> local-file java.io.File. .getName) 
                       remote-file (str hdfs-dir "/" internal-topic "/" dt "/" hr "/" file-name) ;construct the remote file-name
                       ]
                   (go 
                     (>! c [ds local-file remote-file]) ;here id is the local file
                     )))))
    ]
     (register (->Processor "hdfs" start stop exec))
    )
)
(use 'pseidon.hdfs.cloudera.core.hdfsprocessor :reload)
(let [[a b :as c] [1 2]] (prn c))
(use 'pseidon.hdfs.cloudera.core.hdfsprocessor :reload)
(letfn [ (a [] )] (a))
(use 'pseidon.hdfs.cloudera.core.hdfsprocessor :reload)
(def f (fn [x] ))
(def f1 (fn [] ))
(meta f)
(meta 'f)
(meta 'f1)
(meta f1)
(:arglist (meta f1))
(type f)
(class f)
(-> (class f) .getMethods)
(-> (class f) .getMethods (map str))
(->> (class f) .getMethods (map str))
(f)
(f 1)
(f1)
(f1 7)
(throw ArithmeticException "hi")
(throw (ArithmeticException. ))
(throw (ArityException.))
(throw (clojure.lang.ArityException.))
(throw (clojure.lang.ArityException. ""))
(throw (clojure.lang.ArityException. "12"))
(use 'pseidon.hdfs.cloudera.core.hdfsprocessor)
(import 'java.io.File)
(-> "/tmp/myfile.txt" File. .getName)
(-> "/tmp/myfile.txt" File. .getAbsoluteName)
(-> "/tmp/myfile.txt" File. .getAbsolutePath)
(doc set-ref)
(doc ref-set)
(def a [s] (str "apply a to " s))
(defn a [s] (str "apply a to " s))
(defn b [s] (str "apply b to " s))
(defmarco apply-f [model]
   (cons (= model 1)
         `a)) 
(defmarco apply-f [model arg]
(defmacro apply-f [model]
   (cons (= model 1)
         ))
(defmacro apply-f [model arg]
   (cons (= model 1)
          `(a ~arg)
         (= model 2)
          `(b ~arg)
))
(appply-f 1 2)
(apply-f 1 2)
(apply-f 1 )
(apply-f )
(apply-f 1 2)
(defmacro apply-f [model & arg]
)
(defmacro apply-model [model-key model-map & args]
  (if-let [f (get model-map model-key)]
   `(f ~@args)))
(apply-model 1 {1 (fn [x] (str "hi " x))} "Gerrit")
(defmacro apply-model [model-key model-map & args]
  (if-let [f# (get model-map model-key)]
   `(f# ~@args)))
(apply-model 1 {1 (fn [x] (str "hi " x))} "Gerrit")
(defmacro apply-model [model-key model-map & args]
  (if-let [f# (get model-map model-key)]
   `(~f# ~@args)))
(apply-model 1 {1 (fn [x] (str "hi " x))} "Gerrit")
(def ^:dynamic dt-formatter (formatter "yyyyMMdd"))
(def ^:dynamic hr-formatter (formatter "yyyyMMddHH"))
(use 'clj-time.format)
(def ^:dynamic dt-formatter (formatter "yyyyMMdd"))
(def ^:dynamic hr-formatter (formatter "yyyyMMddHH"))
(def d (parse hr-formatter "2013090212"))
d
(year d)
(use 'clj-time.core)
(hour d)
(year d)
(day d)
(day-of-week d)
(days d)
(doc clojure.java.io/delete-file)
(clojure.java.io/delete-file "avacd.txt" true)
(clojure.java.io/delete-file "avacd.txt" )
(clojure.java.io/delete-file "avacd.txt" false)
(use 'pseidon.hdfs.cloudera.core.hdfsprocessor)
dt-formatter
(use 'pseidon.hdfs.cloudera.core.hdfsprocessor :reload)
dt-formatter
(use 'pseidon.hdfs.cloudera.core.hdfsprocessor :reload)
(use 'pseidon.hdfs.cloudera.core.hdfsprocessor)
(def ^:dynamic file-name-parsers {1 
                                     (fn [file-name]
                                       "Expects a file name with type_id_hr_yyyyMMddHH.extension
                                        use this method as (let [ [type id _ date] (parse-file-name file-name)]  ) 
 "
(let [[type id _ date] (clojure.string/split file-name #"[_\.]")]
 (info "Parsing file " file-name " to [ " type " " id " " date "]")
           [type id nil date]))
                                     2 (fn [file-name]
                                        "Expects a file name with type_yyyMMddHH.extension
                                         the id value part is returned empty
                                         " 
                                         (let [ [type date] (clojure.string/split file-name #"[_\.]")]
                                           [type "" date]))
                                     })
(use 'clojure.tools.logging)
 (fn [file-name]
                                       "Expects a file name with type_id_hr_yyyyMMddHH.extension
                                        use this method as (let [ [type id _ date] (parse-file-name file-name)]  ) 
 "
(let [[type id _ date] (clojure.string/split file-name #"[_\.]")]
 (info "Parsing file " file-name " to [ " type " " id " " date "]")
           [type id nil date]))
(def f1  (fn [file-name]
                                       "Expects a file name with type_id_hr_yyyyMMddHH.extension
                                        use this method as (let [ [type id _ date] (parse-file-name file-name)]  ) 
 "
(let [[type id _ date] (clojure.string/split file-name #"[_\.]")]
 (info "Parsing file " file-name " to [ " type " " id " " date "]")
           [type id nil date])))
(use 'clj-time.coerce)
(use 'clj-time.core)
(def ^:dynamic dt-formatter (formatter "yyyyMMdd"))
(def ^:dynamic hr-formatter (formatter "yyyyMMddHH"))
(use 'clj-time.format)
(def ^:dynamic dt-formatter (formatter "yyyyMMdd"))
(def ^:dynamic hr-formatter (formatter "yyyyMMddHH"))
(def file "advertiser-banner-impressions.2013091909.44735510003754505.gz")
(f1 file)
(def file1 "advertiser-banner-impressions.myid.2013091909.44735510003754505.gz")
(f1 file1)
(re-find #"\d\d\d\d\d\d\d\d\d\d" file)
(re-find #"\d\d\d\d\d\d\d\d\d\d" file1)
(re-find #"\d{10}" file1)
(re-find #"\d{9}" file1)
(re-find #"\d{11}" file1)
(re-find #"\d{10}" file1)
(re-find #"\d{11111}" file1)
(re-find #"\d{4}-\d{2}-\d{2}-\d{2}" "jdkljakfd.jkdfjdksjf.jkdajfdksj.1212231.2013-09-20-09")
(doc clojure.string/replace)
file
(def file "hdljaflsdf.,dkjdsjf.akdfjksjfdks.2013-09-20-20.jkdjfal.jkldaf.12123213")
(-> file  (re-find #"\d{4}-\d{2}-\d{2}-\d{2}") (clojure.string/replace #"-" "") )
(clojure.string/replace (re-find #"\d{4}-\d{2}-\d{2}-\d{2}") #"-" "")
(clojure.string/replace "hl" #"h" "")
(type (re-find #"\d{4}-\d{2}-\d{2}-\d{2}"))
(clojure.string/replace (re-find #"\d{4}-\d{2}-\d{2}-\d{2}" file) #"-" "")
quit
(use 'pseidon.hdfs.cloudera.core.hdfsprocessor)
(use 'pseidon.hdfs.core.hdfsprocessor)
(use 'pseidon.hdfs.core.hdfsprocessor :reload)
(use '[clojure.data.json :as json])
(require '[clojure.data.json :as json])
(def s (json/read-str "[[\"a/b/c\" \"1\"]]"))
s
(get-in {:a {:b 1}} [a b] 1)
(get-in {:a {:b 1}} [:a :b] )
(get-in {:a {:b 1}} [:a :c] )
(get-in {:a {:b 1}} [:a :c] 2)
(clojure.string/split "a/b/" #"/")
(clojure.string/join [a b] #",")
(clojure.string/join [a b] ",")
(clojure.string/join [:a :b] ",")
(clojure.string/join "," [:a :b] )
(doc clojure.string/split)
(doc clojure.string/join)
(get-in {:a {:b 1}} [] 2)
(get-in {:a {:b 1}} [-1] 2)
(clojure.string/split "a/b/" #"/")
(clojure.string/split "/a/b/" #"/")
(clojure.string/split "//a/b/" #"/")
(clojure.string/split "//a/b//" #"/")
(clojure.string/split "//a/b//" #"/" 1)
(clojure.string/split "//a/b//" #"/" 2)
(clojure.string/split "//a/b//" #"/" 3)
(clojure.string/split "//a//b//" #"/" 3)
(empty? "")
(empty? " ")
(empty? "  ")
(empty? "")
(not-empty "")
(not-empty " ")
(conj 1 [])
(conj [] 1)
(conj [1] 2)
(conj [1 2 3] 4)
(use 'pseidon.hdfs.core.json-csv :reload)
(parse-col-def ["/a/b/c" 1])
(.printStackTrace *E)
(parse-col-def ["/a/b/c" 1])
(.printStackTrace *e)
(use 'pseidon.hdfs.core.json-csv :reload)
(parse-col-def ["/a/b/c" 1])
(use 'pseidon.hdfs.core.json-csv :reload)
(parse-col-def ["/a/b/c" 1])
(use 'pseidon.hdfs.core.json-csv :reload)
(parse-col-def ["/a/b/c" 1])
(use 'pseidon.hdfs.core.json-csv :reload)
(parse-col-def ["/a/b/c" 1])
(use 'pseidon.hdfs.core.json-csv :reload)
(parse-col-def ["/a/b/c" 1])
(use 'pseidon.hdfs.core.json-csv :reload)
(parse-col-def ["/a/b/c" 1])
(use 'pseidon.hdfs.core.json-csv :reload)
(parse-col-def ["/a/b/c" 1])
(parse-col-def ["//a/b/c" 1])
(parse-col-def ["/////a/b/c" 1])
(parse-col-def ["//////a/b/c" 1])
(parse-col-def ["///////a/b/c" 1])
(use 'pseidon.hdfs.core.json-csv :reload)
(parse-definitions [["a/b" 1] ["c/d" 2] ["e/f/g" 3]])
(parse-definitions "[[\"a/b\" 1] [\"c/d\" 2] [\"e/f/g\" 3]]")
(def v (parse-definitions "[[\"a/b\" 1] [\"c/d\" 2] [\"e/f/g\" 3]]"))
v
(def v (parse-definitions "[\"a/b\" 1] [\"c/d\" 2] [\"e/f/g\" 3]"))
(def v (parse-definitions "[[\"a/b\" 1] [\"c/d\" 2] [\"e/f/g\" 3]]"))
v
(v 0)
(get v 0)
(get v 1)
(nth v 0)
(vector v)
(vect v)
(vec v)
(use 'pseidon.hdfs.core.json-csv :reload)
(def v (parse-definitions "[[\"a/b\" 1] [\"c/d\" 2] [\"e/f/g\" 3]]"))
(v 0)
(v 1)
(v 2)
v
(json->array v {})
(json->csv v {})
(json->csv v "," {})
(use 'pseidon.hdfs.core.partition-updater)
(use 'pseidon.hdfs.core.partition-updater :reload)
(require '[clojure.java.shell :as sh])
(sh "ls -lh")
(sh/sh "ls" "-lh")
(use 'pseidon.hdfs.core.partition-updater :reload)
(log-error {:error "hi" :exit 1})
(use 'pseidon.hdfs.core.partition-updater :reload)
(log-error {:error "hi" :exit 1})
(log-error #({:error "hi" :exit 1}))
(log-error (fn [] {:error "hi" :exit 1}))
(use 'pseidon.hdfs.core.partition-updater :reload)
(log-error (fn [] {:error "hi" :exit 1}))
(log-error (fn [] {:error "hi" :exit 0}))
(use 'midje.repl)
(autotest)
(use 'midje.repl)
(autotest)
(.printStackTrace *me)
(use 'midje.repl)
(autotest)
(use 'pseidon.hdfs.core.hdfsprocessor)
(require '[clojure.core.async :as a])
a/go-loop
(require '[clojure.core.async :as a])
a/go-loop
(require '[clojure.core.async :as a])
a/go-loop
a/go
clojure.core.async/go-loop
(use 'pseidon.hdfs.core.hdfsprocessor)
(.printStackTrace *e)
(use 'pseidon.hdfs.core.hdfsprocessor)
(use 'clojure.core.async)
go
(doc go)
(use 'clojure.core.async)
(doc go)
(use 'clojure.core.async)
