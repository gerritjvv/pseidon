readlink: illegal option -- f
usage: readlink [-n] [file ...]
Warning: JAVA_HOME environment variable is not set.
[INFO] Scanning for projects...
[INFO] 
[INFO] Using the builder org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder with a thread count of 1
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building pseidon-hdfs 0.1.0
[INFO] ------------------------------------------------------------------------
[WARNING] The POM for asm:asm:jar:3.1 is invalid, transitive dependencies (if any) will not be available, enable debug logging for more details
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ pseidon-hdfs ---
[INFO] Deleting /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/target
[INFO] 
[INFO] --- build-helper-maven-plugin:1.7:add-source (default) @ pseidon-hdfs ---
[INFO] Source directory: /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/java added.
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ pseidon-hdfs ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] skip non existing resourceDirectory /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ pseidon-hdfs ---
[INFO] No sources to compile
[INFO] 
[INFO] --- clojure-maven-plugin:1.7.1:compile (compile) @ pseidon-hdfs ---
Compiling pseidon-hdfs.lifecycle to /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/target/classes
Compiling pseidon-hdfs.db-service to /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/target/classes
Compiling pseidon-hdfs.metrics to /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/target/classes
Compiling pseidon-hdfs.pseidon-hdfs to /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/target/classes
Compiling pseidon-hdfs.util to /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/target/classes
Compiling pseidon-hdfs.app to /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/target/classes
Compiling pseidon-hdfs.hive to /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/target/classes
Compiling pseidon-hdfs.hdfs-copy-service to /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/target/classes
Compiling pseidon-hdfs.nrepl-service to /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/target/classes
Compiling pseidon-hdfs.conf to /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/target/classes
Compiling pseidon-hdfs.mon to /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/target/classes
Compiling pseidon-hdfs.hdfs-util to /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/target/classes
Compiling pseidon-hdfs.watchdog to /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ pseidon-hdfs ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] skip non existing resourceDirectory /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ pseidon-hdfs ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ pseidon-hdfs ---
[INFO] No tests to run.
[INFO] 
[INFO] --- clojure-maven-plugin:1.7.1:test (test-clojure) @ pseidon-hdfs ---
WARNING: cat already refers to: #'clojure.core/cat in namespace: net.cgrand.parsley.fold, being replaced by: #'net.cgrand.parsley.fold/cat
WARNING: update already refers to: #'clojure.core/update in namespace: utilize.map, being replaced by: #'utilize.map/update
WARNING: update already refers to: #'clojure.core/update in namespace: clojure.math.combinatorics, being replaced by: #'clojure.math.combinatorics/update
 WARN main org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Formatting using clusterid: testClusterID
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - No KeyProvider found.
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - fsLock is fair:true
 INFO main org.apache.hadoop.conf.Configuration.deprecation - hadoop.configured.node.mapping is deprecated. Instead, use net.topology.configured.node.mapping
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager - dfs.block.invalidate.limit=1000
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager - dfs.namenode.datanode.registration.ip-hostname-check=true
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - The block deletion will start around 2016 Nov 30 00:56:27
 INFO main org.apache.hadoop.util.GSet - Computing capacity for map BlocksMap
 INFO main org.apache.hadoop.util.GSet - VM type       = 64-bit
 INFO main org.apache.hadoop.util.GSet - 2.0% max memory 3.6 GB = 72.8 MB
 INFO main org.apache.hadoop.util.GSet - capacity      = 2^23 = 8388608 entries
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - dfs.block.access.token.enable=false
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - defaultReplication         = 1
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - maxReplication             = 512
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - minReplication             = 1
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - maxReplicationStreams      = 2
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - replicationRecheckInterval = 3000
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - encryptDataTransfer        = false
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - maxNumBlocksToLog          = 1000
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - fsOwner             = gvanvuuren (auth:SIMPLE)
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - supergroup          = supergroup
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - isPermissionEnabled = true
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - HA Enabled: false
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Append Enabled: true
 INFO main org.apache.hadoop.util.GSet - Computing capacity for map INodeMap
 INFO main org.apache.hadoop.util.GSet - VM type       = 64-bit
 INFO main org.apache.hadoop.util.GSet - 1.0% max memory 3.6 GB = 36.4 MB
 INFO main org.apache.hadoop.util.GSet - capacity      = 2^22 = 4194304 entries
 INFO main org.apache.hadoop.hdfs.server.namenode.NameNode - Caching file names occuring more than 10 times
 INFO main org.apache.hadoop.util.GSet - Computing capacity for map cachedBlocks
 INFO main org.apache.hadoop.util.GSet - VM type       = 64-bit
 INFO main org.apache.hadoop.util.GSet - 0.25% max memory 3.6 GB = 9.1 MB
 INFO main org.apache.hadoop.util.GSet - capacity      = 2^20 = 1048576 entries
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - dfs.namenode.safemode.min.datanodes = 0
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - dfs.namenode.safemode.extension     = 0
 INFO main org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics - NNTop conf: dfs.namenode.top.window.num.buckets = 10
 INFO main org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics - NNTop conf: dfs.namenode.top.num.users = 10
 INFO main org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Retry cache on namenode is enabled
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
 INFO main org.apache.hadoop.util.GSet - Computing capacity for map NameNodeRetryCache
 INFO main org.apache.hadoop.util.GSet - VM type       = 64-bit
 INFO main org.apache.hadoop.util.GSet - 0.029999999329447746% max memory 3.6 GB = 1.1 MB
 INFO main org.apache.hadoop.util.GSet - capacity      = 2^17 = 131072 entries
 INFO main org.apache.hadoop.hdfs.server.namenode.NNConf - ACLs enabled? false
 INFO main org.apache.hadoop.hdfs.server.namenode.NNConf - XAttrs enabled? true
 INFO main org.apache.hadoop.hdfs.server.namenode.NNConf - Maximum size of an xattr: 16384
 INFO main org.apache.hadoop.hdfs.server.namenode.FSImage - Allocated new BlockPoolId: BP-12931932-10.0.0.10-1480463788100
 INFO main org.apache.hadoop.hdfs.server.common.Storage - Storage directory /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/name1 has been successfully formatted.
 INFO main org.apache.hadoop.hdfs.server.common.Storage - Storage directory /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/name2 has been successfully formatted.
 INFO main org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager - Going to retain 1 images with txid >= 0
 INFO main org.apache.hadoop.hdfs.server.namenode.NameNode - createNameNode []
 WARN main org.apache.hadoop.metrics2.impl.MetricsConfig - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
 INFO main org.apache.hadoop.metrics2.impl.MetricsSystemImpl - Scheduled snapshot period at 10 second(s).
 INFO main org.apache.hadoop.metrics2.impl.MetricsSystemImpl - NameNode metrics system started
 INFO main org.apache.hadoop.hdfs.server.namenode.NameNode - fs.defaultFS is hdfs://127.0.0.1:0
 INFO org.apache.hadoop.util.JvmPauseMonitor$Monitor@1add3e03 org.apache.hadoop.util.JvmPauseMonitor - Starting JVM pause monitor
 INFO main org.apache.hadoop.hdfs.DFSUtil - Starting Web-server for hdfs at: http://localhost:0
 INFO main org.mortbay.log - Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
 INFO main org.apache.hadoop.security.authentication.server.AuthenticationFilter - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
 INFO main org.apache.hadoop.http.HttpRequestLog - Http request log for http.requests.namenode is not defined
 INFO main org.apache.hadoop.http.HttpServer2 - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
 INFO main org.apache.hadoop.http.HttpServer2 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
 INFO main org.apache.hadoop.http.HttpServer2 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
 INFO main org.apache.hadoop.http.HttpServer2 - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
 INFO main org.apache.hadoop.http.HttpServer2 - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
 INFO main org.apache.hadoop.http.HttpServer2 - Jetty bound to port 54991
 INFO main org.mortbay.log - jetty-6.1.26.cloudera.4
 INFO main org.mortbay.log - Extract jar:file:/Users/gvanvuuren/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar!/webapps/hdfs to /var/folders/tk/w8__4rl57830l94b5y2l7g6w0000gn/T/Jetty_localhost_54991_hdfs____5rax8c/webapp
 INFO main org.mortbay.log - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:54991
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - No KeyProvider found.
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - fsLock is fair:true
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager - dfs.block.invalidate.limit=1000
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager - dfs.namenode.datanode.registration.ip-hostname-check=true
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - The block deletion will start around 2016 Nov 30 00:56:28
 INFO main org.apache.hadoop.util.GSet - Computing capacity for map BlocksMap
 INFO main org.apache.hadoop.util.GSet - VM type       = 64-bit
 INFO main org.apache.hadoop.util.GSet - 2.0% max memory 3.6 GB = 72.8 MB
 INFO main org.apache.hadoop.util.GSet - capacity      = 2^23 = 8388608 entries
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - dfs.block.access.token.enable=false
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - defaultReplication         = 1
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - maxReplication             = 512
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - minReplication             = 1
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - maxReplicationStreams      = 2
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - replicationRecheckInterval = 3000
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - encryptDataTransfer        = false
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - maxNumBlocksToLog          = 1000
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - fsOwner             = gvanvuuren (auth:SIMPLE)
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - supergroup          = supergroup
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - isPermissionEnabled = true
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - HA Enabled: false
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Append Enabled: true
 INFO main org.apache.hadoop.util.GSet - Computing capacity for map INodeMap
 INFO main org.apache.hadoop.util.GSet - VM type       = 64-bit
 INFO main org.apache.hadoop.util.GSet - 1.0% max memory 3.6 GB = 36.4 MB
 INFO main org.apache.hadoop.util.GSet - capacity      = 2^22 = 4194304 entries
 INFO main org.apache.hadoop.hdfs.server.namenode.NameNode - Caching file names occuring more than 10 times
 INFO main org.apache.hadoop.util.GSet - Computing capacity for map cachedBlocks
 INFO main org.apache.hadoop.util.GSet - VM type       = 64-bit
 INFO main org.apache.hadoop.util.GSet - 0.25% max memory 3.6 GB = 9.1 MB
 INFO main org.apache.hadoop.util.GSet - capacity      = 2^20 = 1048576 entries
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - dfs.namenode.safemode.min.datanodes = 0
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - dfs.namenode.safemode.extension     = 0
 INFO main org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics - NNTop conf: dfs.namenode.top.window.num.buckets = 10
 INFO main org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics - NNTop conf: dfs.namenode.top.num.users = 10
 INFO main org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Retry cache on namenode is enabled
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
 INFO main org.apache.hadoop.util.GSet - Computing capacity for map NameNodeRetryCache
 INFO main org.apache.hadoop.util.GSet - VM type       = 64-bit
 INFO main org.apache.hadoop.util.GSet - 0.029999999329447746% max memory 3.6 GB = 1.1 MB
 INFO main org.apache.hadoop.util.GSet - capacity      = 2^17 = 131072 entries
 INFO main org.apache.hadoop.hdfs.server.namenode.NNConf - ACLs enabled? false
 INFO main org.apache.hadoop.hdfs.server.namenode.NNConf - XAttrs enabled? true
 INFO main org.apache.hadoop.hdfs.server.namenode.NNConf - Maximum size of an xattr: 16384
 INFO main org.apache.hadoop.hdfs.server.common.Storage - Lock on /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/name1/in_use.lock acquired by nodename 9517@Gerrrits-MacBook-Pro.local
 INFO main org.apache.hadoop.hdfs.server.common.Storage - Lock on /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/name2/in_use.lock acquired by nodename 9517@Gerrrits-MacBook-Pro.local
 INFO main org.apache.hadoop.hdfs.server.namenode.FileJournalManager - Recovering unfinalized segments in /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/name1/current
 INFO main org.apache.hadoop.hdfs.server.namenode.FileJournalManager - Recovering unfinalized segments in /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/name2/current
 INFO main org.apache.hadoop.hdfs.server.namenode.FSImage - No edit log streams selected.
 INFO main org.apache.hadoop.hdfs.server.namenode.FSImage - Planning to load image: FSImageFile(file=/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/name1/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
 INFO main org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode - Loading 1 INodes.
 INFO main org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf - Loaded FSImage in 0 seconds.
 INFO main org.apache.hadoop.hdfs.server.namenode.FSImage - Loaded image for txid 0 from /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/name1/current/fsimage_0000000000000000000
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
 INFO main org.apache.hadoop.hdfs.server.namenode.FSEditLog - Starting log segment at 1
 INFO main org.apache.hadoop.hdfs.server.namenode.NameCache - initialized with 0 entries 0 lookups
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Finished loading FSImage in 53 msecs
 INFO main org.apache.hadoop.hdfs.server.namenode.NameNode - RPC server is binding to localhost:0
 INFO main org.apache.hadoop.ipc.CallQueueManager - Using callQueue class java.util.concurrent.LinkedBlockingQueue
 INFO Socket Reader #1 for port 54992 org.apache.hadoop.ipc.Server - Starting Socket Reader #1 for port 54992
 INFO main org.apache.hadoop.hdfs.server.namenode.NameNode - Clients are to use localhost:54992 to access this namenode/service.
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Registered FSNamesystemState MBean
 INFO main org.apache.hadoop.hdfs.server.namenode.LeaseManager - Number of blocks under construction: 0
 INFO main org.apache.hadoop.hdfs.server.namenode.LeaseManager - Number of blocks under construction: 0
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - initializing replication queues
 INFO main org.apache.hadoop.hdfs.StateChange - STATE* Leaving safe mode after 0 secs
 INFO main org.apache.hadoop.hdfs.StateChange - STATE* Network topology has 0 racks and 0 datanodes
 INFO main org.apache.hadoop.hdfs.StateChange - STATE* UnderReplicatedBlocks has 0 blocks
 INFO Replication Queue Initializer org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - Total number of blocks            = 0
 INFO Replication Queue Initializer org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - Number of invalid blocks          = 0
 INFO Replication Queue Initializer org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - Number of under-replicated blocks = 0
 INFO Replication Queue Initializer org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - Number of  over-replicated blocks = 0
 INFO Replication Queue Initializer org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - Number of blocks being written    = 0
 INFO Replication Queue Initializer org.apache.hadoop.hdfs.StateChange - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 7 msec
 INFO IPC Server Responder org.apache.hadoop.ipc.Server - IPC Server Responder: starting
 INFO IPC Server listener on 54992 org.apache.hadoop.ipc.Server - IPC Server listener on 54992: starting
 INFO main org.apache.hadoop.hdfs.server.namenode.NameNode - NameNode RPC up at: localhost/127.0.0.1:54992
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Starting services required for active state
 INFO CacheReplicationMonitor(1074252268) org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor - Starting CacheReplicationMonitor with interval 30000 milliseconds
 INFO CacheReplicationMonitor(1074252268) org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor - Rescanning after 137599711 milliseconds
 INFO CacheReplicationMonitor(1074252268) org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor - Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
 INFO main org.apache.hadoop.hdfs.MiniDFSCluster - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1,[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2
 INFO main org.apache.hadoop.metrics2.impl.MetricsSystemImpl - DataNode metrics system started (again)
 INFO main org.apache.hadoop.hdfs.server.datanode.BlockScanner - Initialized block scanner with targetBytesPerSec 1048576
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - Configured hostname is 127.0.0.1
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - Starting DataNode with maxLockedMemory = 0
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - Opened streaming server at /127.0.0.1:54993
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - Balancing bandwith is 1048576 bytes/s
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - Number threads for balancing is 5
 INFO main org.apache.hadoop.security.authentication.server.AuthenticationFilter - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
 INFO main org.apache.hadoop.http.HttpRequestLog - Http request log for http.requests.datanode is not defined
 INFO main org.apache.hadoop.http.HttpServer2 - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
 INFO main org.apache.hadoop.http.HttpServer2 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
 INFO main org.apache.hadoop.http.HttpServer2 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
 INFO main org.apache.hadoop.http.HttpServer2 - Jetty bound to port 54994
 INFO main org.mortbay.log - jetty-6.1.26.cloudera.4
 INFO main org.mortbay.log - Extract jar:file:/Users/gvanvuuren/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar!/webapps/datanode to /var/folders/tk/w8__4rl57830l94b5y2l7g6w0000gn/T/Jetty_localhost_54994_datanode____.akzypw/webapp
 INFO main org.mortbay.log - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:54994
 INFO main org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer - Listening HTTP traffic on /127.0.0.1:54995
 INFO org.apache.hadoop.util.JvmPauseMonitor$Monitor@7ca7bddd org.apache.hadoop.util.JvmPauseMonitor - Starting JVM pause monitor
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - dnUserName = gvanvuuren
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - supergroup = supergroup
 INFO main org.apache.hadoop.ipc.CallQueueManager - Using callQueue class java.util.concurrent.LinkedBlockingQueue
 INFO Socket Reader #1 for port 54996 org.apache.hadoop.ipc.Server - Starting Socket Reader #1 for port 54996
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - Opened IPC server at /127.0.0.1:54996
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - Refresh request received for nameservices: null
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - Starting BPOfferServices for nameservices: <default>
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.datanode.DataNode - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:54992 starting to offer service
 INFO IPC Server Responder org.apache.hadoop.ipc.Server - IPC Server Responder: starting
 INFO IPC Server listener on 54996 org.apache.hadoop.ipc.Server - IPC Server listener on 54996: starting
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.common.Storage - Lock on /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/in_use.lock acquired by nodename 9517@Gerrrits-MacBook-Pro.local
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.common.Storage - Storage directory /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1 is not formatted for BP-12931932-10.0.0.10-1480463788100
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.common.Storage - Formatting ...
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.common.Storage - Analyzing storage directories for bpid BP-12931932-10.0.0.10-1480463788100
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.common.Storage - Locking is disabled for /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/current/BP-12931932-10.0.0.10-1480463788100
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.common.Storage - Block pool storage directory /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/current/BP-12931932-10.0.0.10-1480463788100 is not formatted for BP-12931932-10.0.0.10-1480463788100
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.common.Storage - Formatting ...
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.common.Storage - Formatting block pool BP-12931932-10.0.0.10-1480463788100 directory /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/current/BP-12931932-10.0.0.10-1480463788100/current
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.common.Storage - Lock on /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/in_use.lock acquired by nodename 9517@Gerrrits-MacBook-Pro.local
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.common.Storage - Storage directory /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2 is not formatted for BP-12931932-10.0.0.10-1480463788100
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.common.Storage - Formatting ...
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.common.Storage - Analyzing storage directories for bpid BP-12931932-10.0.0.10-1480463788100
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.common.Storage - Locking is disabled for /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/current/BP-12931932-10.0.0.10-1480463788100
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.common.Storage - Block pool storage directory /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/current/BP-12931932-10.0.0.10-1480463788100 is not formatted for BP-12931932-10.0.0.10-1480463788100
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.common.Storage - Formatting ...
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.common.Storage - Formatting block pool BP-12931932-10.0.0.10-1480463788100 directory /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/current/BP-12931932-10.0.0.10-1480463788100/current
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.datanode.DataNode - Setting up storage: nsid=1561369271;bpid=BP-12931932-10.0.0.10-1480463788100;lv=-56;nsInfo=lv=-60;cid=testClusterID;nsid=1561369271;c=0;bpid=BP-12931932-10.0.0.10-1480463788100;dnuuid=null
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.datanode.DataNode - Generated and persisted new Datanode UUID bc889bf9-a333-4d69-81ac-93b4c29fb564
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Added new volume: DS-95faf3fb-3733-49e6-817a-49683977e493
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Added volume - /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/current, StorageType: DISK
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Added new volume: DS-3f394624-ac8a-4c16-a924-f3d2e7474855
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Added volume - /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/current, StorageType: DISK
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Registered FSDatasetState MBean
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Volume reference is released.
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Adding block pool BP-12931932-10.0.0.10-1480463788100
 INFO Thread-68 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Scanning block pool BP-12931932-10.0.0.10-1480463788100 on volume /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/current...
 INFO Thread-69 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Scanning block pool BP-12931932-10.0.0.10-1480463788100 on volume /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/current...
 INFO main org.apache.hadoop.hdfs.MiniDFSCluster - dnInfo.length != numDataNodes
 INFO main org.apache.hadoop.hdfs.MiniDFSCluster - Waiting for cluster to become active
 INFO Thread-68 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Time taken to scan block pool BP-12931932-10.0.0.10-1480463788100 on /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/current: 14ms
 INFO Thread-69 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Time taken to scan block pool BP-12931932-10.0.0.10-1480463788100 on /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/current: 14ms
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Total time to scan all replicas for block pool BP-12931932-10.0.0.10-1480463788100: 15ms
 INFO Thread-72 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Adding replicas to map for block pool BP-12931932-10.0.0.10-1480463788100 on volume /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/current...
 INFO Thread-73 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Adding replicas to map for block pool BP-12931932-10.0.0.10-1480463788100 on volume /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/current...
 INFO Thread-72 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Time to add replicas to map for block pool BP-12931932-10.0.0.10-1480463788100 on volume /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/current: 0ms
 INFO Thread-73 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Time to add replicas to map for block pool BP-12931932-10.0.0.10-1480463788100 on volume /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/current: 0ms
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Total time to add all replicas to map: 1ms
 INFO VolumeScannerThread(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2) org.apache.hadoop.hdfs.server.datanode.VolumeScanner - Now scanning bpid BP-12931932-10.0.0.10-1480463788100 on volume /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2
 INFO VolumeScannerThread(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1) org.apache.hadoop.hdfs.server.datanode.VolumeScanner - Now scanning bpid BP-12931932-10.0.0.10-1480463788100 on volume /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.datanode.DirectoryScanner - Periodic Directory Tree Verification scan starting at 1480481148713ms with interval of 21600000ms
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.datanode.DataNode - Block pool BP-12931932-10.0.0.10-1480463788100 (Datanode Uuid null) service to localhost/127.0.0.1:54992 beginning handshake with NN
 INFO VolumeScannerThread(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1) org.apache.hadoop.hdfs.server.datanode.VolumeScanner - VolumeScanner(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1, DS-95faf3fb-3733-49e6-817a-49683977e493): finished scanning block pool BP-12931932-10.0.0.10-1480463788100
 INFO IPC Server handler 2 on 54992 org.apache.hadoop.hdfs.StateChange - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=bc889bf9-a333-4d69-81ac-93b4c29fb564, infoPort=54995, infoSecurePort=0, ipcPort=54996, storageInfo=lv=-56;cid=testClusterID;nsid=1561369271;c=0) storage bc889bf9-a333-4d69-81ac-93b4c29fb564
 INFO VolumeScannerThread(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2) org.apache.hadoop.hdfs.server.datanode.VolumeScanner - VolumeScanner(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2, DS-3f394624-ac8a-4c16-a924-f3d2e7474855): finished scanning block pool BP-12931932-10.0.0.10-1480463788100
 INFO IPC Server handler 2 on 54992 org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor - Number of failed storage changes from 0 to 0
 INFO IPC Server handler 2 on 54992 org.apache.hadoop.net.NetworkTopology - Adding a new node: /default-rack/127.0.0.1:54993
 INFO IPC Server handler 2 on 54992 org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager - Registered DN bc889bf9-a333-4d69-81ac-93b4c29fb564 (127.0.0.1:54993).
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.datanode.DataNode - Block pool Block pool BP-12931932-10.0.0.10-1480463788100 (Datanode Uuid null) service to localhost/127.0.0.1:54992 successfully registered with NN
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.datanode.DataNode - For namenode localhost/127.0.0.1:54992 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
 INFO IPC Server handler 3 on 54992 org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor - Number of failed storage changes from 0 to 0
 INFO IPC Server handler 3 on 54992 org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor - Adding new storage ID DS-95faf3fb-3733-49e6-817a-49683977e493 for DN 127.0.0.1:54993
 INFO IPC Server handler 3 on 54992 org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor - Adding new storage ID DS-3f394624-ac8a-4c16-a924-f3d2e7474855 for DN 127.0.0.1:54993
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.datanode.DataNode - Namenode Block pool BP-12931932-10.0.0.10-1480463788100 (Datanode Uuid bc889bf9-a333-4d69-81ac-93b4c29fb564) service to localhost/127.0.0.1:54992 trying to claim ACTIVE state with txid=1
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.datanode.DataNode - Acknowledging ACTIVE Namenode Block pool BP-12931932-10.0.0.10-1480463788100 (Datanode Uuid bc889bf9-a333-4d69-81ac-93b4c29fb564) service to localhost/127.0.0.1:54992
 INFO IPC Server handler 4 on 54992 org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - Processing first storage report for DS-95faf3fb-3733-49e6-817a-49683977e493 from datanode bc889bf9-a333-4d69-81ac-93b4c29fb564
 INFO IPC Server handler 4 on 54992 BlockStateChange - BLOCK* processReport: from storage DS-95faf3fb-3733-49e6-817a-49683977e493 node DatanodeRegistration(127.0.0.1, datanodeUuid=bc889bf9-a333-4d69-81ac-93b4c29fb564, infoPort=54995, infoSecurePort=0, ipcPort=54996, storageInfo=lv=-56;cid=testClusterID;nsid=1561369271;c=0), blocks: 0, hasStaleStorage: true, processing time: 1 msecs
 INFO IPC Server handler 4 on 54992 org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - Processing first storage report for DS-3f394624-ac8a-4c16-a924-f3d2e7474855 from datanode bc889bf9-a333-4d69-81ac-93b4c29fb564
 INFO IPC Server handler 4 on 54992 BlockStateChange - BLOCK* processReport: from storage DS-3f394624-ac8a-4c16-a924-f3d2e7474855 node DatanodeRegistration(127.0.0.1, datanodeUuid=bc889bf9-a333-4d69-81ac-93b4c29fb564, infoPort=54995, infoSecurePort=0, ipcPort=54996, storageInfo=lv=-56;cid=testClusterID;nsid=1561369271;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
 INFO VolumeScannerThread(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2) org.apache.hadoop.hdfs.server.datanode.VolumeScanner - VolumeScanner(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2, DS-3f394624-ac8a-4c16-a924-f3d2e7474855): no suitable block pools found to scan.  Waiting 1814399941 ms.
 INFO VolumeScannerThread(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1) org.apache.hadoop.hdfs.server.datanode.VolumeScanner - VolumeScanner(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1, DS-95faf3fb-3733-49e6-817a-49683977e493): no suitable block pools found to scan.  Waiting 1814399941 ms.
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.datanode.DataNode - Successfully sent block report 0xb52a9fb2287ca931,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 20 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.datanode.DataNode - Got finalize command for block pool BP-12931932-10.0.0.10-1480463788100
 INFO main org.apache.hadoop.hdfs.MiniDFSCluster - Cluster is active
 INFO main org.apache.hadoop.hdfs.MiniDFSCluster - Cluster is active
 INFO main pseidon-hdfs.hdfs-copy-service - copying file  target/test/copyservicetest/1480463789819/test-2016-03-20-09.gz  via  /tmp/copying_log_pseidon_test_dt=20160320_hr=2016032009_Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz  to  /log/pseidon/test/dt=20160320/hr=2016032009/Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz
 INFO IPC Server handler 7 on 54992 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/log/pseidon/test/dt=20160320/hr=2016032009	dst=null	perm=null	proto=rpc
 INFO main pseidon-hdfs.hdfs-util - creating hdfs dir  /log/pseidon/test/dt=20160320/hr=2016032009
 INFO IPC Server handler 8 on 54992 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/log/pseidon/test/dt=20160320/hr=2016032009	dst=null	perm=gvanvuuren:supergroup:rwxr-xr-x	proto=rpc
 INFO main pseidon-hdfs.hdfs-util - calling on-create
 INFO IPC Server handler 9 on 54992 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tmp/copying_log_pseidon_test_dt=20160320_hr=2016032009_Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz	dst=null	perm=null	proto=rpc
 INFO IPC Server handler 0 on 54992 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/tmp/copying_log_pseidon_test_dt=20160320_hr=2016032009_Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz	dst=null	perm=gvanvuuren:supergroup:rw-r--r--	proto=rpc
 INFO IPC Server handler 1 on 54992 org.apache.hadoop.hdfs.StateChange - BLOCK* allocateBlock: /tmp/copying_log_pseidon_test_dt=20160320_hr=2016032009_Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz. BP-12931932-10.0.0.10-1480463788100 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-95faf3fb-3733-49e6-817a-49683977e493:NORMAL:127.0.0.1:54993|RBW]]}
 INFO DataXceiver for client DFSClient_NONMAPREDUCE_-1829303623_1 at /127.0.0.1:54999 [Receiving block BP-12931932-10.0.0.10-1480463788100:blk_1073741825_1001] org.apache.hadoop.hdfs.server.datanode.DataNode - Receiving BP-12931932-10.0.0.10-1480463788100:blk_1073741825_1001 src: /127.0.0.1:54999 dest: /127.0.0.1:54993
 INFO PacketResponder: BP-12931932-10.0.0.10-1480463788100:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace - src: /127.0.0.1:54999, dest: /127.0.0.1:54993, bytes: 7, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1829303623_1, offset: 0, srvID: bc889bf9-a333-4d69-81ac-93b4c29fb564, blockid: BP-12931932-10.0.0.10-1480463788100:blk_1073741825_1001, duration: 16635416
 INFO PacketResponder: BP-12931932-10.0.0.10-1480463788100:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] org.apache.hadoop.hdfs.server.datanode.DataNode - PacketResponder: BP-12931932-10.0.0.10-1480463788100:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
 INFO IPC Server handler 3 on 54992 org.apache.hadoop.hdfs.server.namenode.FSNamesystem - BLOCK* checkFileProgress: blk_1073741825_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-95faf3fb-3733-49e6-817a-49683977e493:NORMAL:127.0.0.1:54993|RBW]]} has not reached minimal replication 1
 INFO IPC Server handler 3 on 54992 org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream - Nothing to flush
 INFO IPC Server handler 3 on 54992 org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream - Nothing to flush
 INFO IPC Server handler 4 on 54992 BlockStateChange - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:54993 is added to blk_1073741825_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-95faf3fb-3733-49e6-817a-49683977e493:NORMAL:127.0.0.1:54993|RBW]]} size 7
 INFO IPC Server handler 5 on 54992 org.apache.hadoop.hdfs.StateChange - DIR* completeFile: /tmp/copying_log_pseidon_test_dt=20160320_hr=2016032009_Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz is closed by DFSClient_NONMAPREDUCE_-1829303623_1
 INFO IPC Server handler 6 on 54992 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=rename	src=/tmp/copying_log_pseidon_test_dt=20160320_hr=2016032009_Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz	dst=/log/pseidon/test/dt=20160320/hr=2016032009/Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz	perm=gvanvuuren:supergroup:rw-r--r--	proto=rpc
 INFO main pseidon-hdfs.hdfs-copy-service - Deleting file  target/test/copyservicetest/1480463789819/test-2016-03-20-09.gz
 INFO main pseidon-hdfs.hdfs-copy-service - copying file  target/test/copyservicetest/1480463789819/test-2016-03-20-09.gz  via  /tmp/copying_log_pseidon_test_dt=20160320_hr=2016032009_Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz  to  /log/pseidon/test/dt=20160320/hr=2016032009/Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz
 INFO IPC Server handler 7 on 54992 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/log/pseidon/test/dt=20160320/hr=2016032009	dst=null	perm=null	proto=rpc
 INFO IPC Server handler 8 on 54992 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tmp/copying_log_pseidon_test_dt=20160320_hr=2016032009_Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz	dst=null	perm=null	proto=rpc
 INFO IPC Server handler 9 on 54992 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/tmp/copying_log_pseidon_test_dt=20160320_hr=2016032009_Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz	dst=null	perm=gvanvuuren:supergroup:rw-r--r--	proto=rpc
 INFO IPC Server handler 0 on 54992 org.apache.hadoop.hdfs.StateChange - BLOCK* allocateBlock: /tmp/copying_log_pseidon_test_dt=20160320_hr=2016032009_Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz. BP-12931932-10.0.0.10-1480463788100 blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-3f394624-ac8a-4c16-a924-f3d2e7474855:NORMAL:127.0.0.1:54993|RBW]]}
 INFO DataXceiver for client DFSClient_NONMAPREDUCE_-1829303623_1 at /127.0.0.1:55000 [Receiving block BP-12931932-10.0.0.10-1480463788100:blk_1073741826_1002] org.apache.hadoop.hdfs.server.datanode.DataNode - Receiving BP-12931932-10.0.0.10-1480463788100:blk_1073741826_1002 src: /127.0.0.1:55000 dest: /127.0.0.1:54993
 INFO PacketResponder: BP-12931932-10.0.0.10-1480463788100:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace - src: /127.0.0.1:55000, dest: /127.0.0.1:54993, bytes: 7, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1829303623_1, offset: 0, srvID: bc889bf9-a333-4d69-81ac-93b4c29fb564, blockid: BP-12931932-10.0.0.10-1480463788100:blk_1073741826_1002, duration: 1936963
 INFO PacketResponder: BP-12931932-10.0.0.10-1480463788100:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] org.apache.hadoop.hdfs.server.datanode.DataNode - PacketResponder: BP-12931932-10.0.0.10-1480463788100:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
 INFO IPC Server handler 2 on 54992 org.apache.hadoop.hdfs.server.namenode.FSNamesystem - BLOCK* checkFileProgress: blk_1073741826_1002{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-3f394624-ac8a-4c16-a924-f3d2e7474855:NORMAL:127.0.0.1:54993|RBW]]} has not reached minimal replication 1
 INFO IPC Server handler 2 on 54992 org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream - Nothing to flush
 INFO IPC Server handler 2 on 54992 org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream - Nothing to flush
 INFO IPC Server handler 1 on 54992 BlockStateChange - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:54993 is added to blk_1073741826_1002{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-3f394624-ac8a-4c16-a924-f3d2e7474855:NORMAL:127.0.0.1:54993|RBW]]} size 7
 INFO IPC Server handler 3 on 54992 org.apache.hadoop.hdfs.StateChange - DIR* completeFile: /tmp/copying_log_pseidon_test_dt=20160320_hr=2016032009_Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz is closed by DFSClient_NONMAPREDUCE_-1829303623_1
 WARN IPC Server handler 4 on 54992 org.apache.hadoop.hdfs.StateChange - DIR* FSDirectory.unprotectedRenameTo: failed to rename /tmp/copying_log_pseidon_test_dt=20160320_hr=2016032009_Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz to /log/pseidon/test/dt=20160320/hr=2016032009/Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz because destination exists
 INFO IPC Server handler 4 on 54992 org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream - Nothing to flush
 INFO IPC Server handler 4 on 54992 org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream - Nothing to flush
 INFO IPC Server handler 5 on 54992 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/log/pseidon/test/dt=20160320/hr=2016032009/Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz	dst=null	perm=null	proto=rpc
 INFO IPC Server handler 6 on 54992 BlockStateChange - BLOCK* addToInvalidates: blk_1073741825_1001 127.0.0.1:54993 
 INFO IPC Server handler 6 on 54992 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=delete	src=/log/pseidon/test/dt=20160320/hr=2016032009/Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz	dst=null	perm=null	proto=rpc
 INFO IPC Server handler 7 on 54992 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=rename	src=/tmp/copying_log_pseidon_test_dt=20160320_hr=2016032009_Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz	dst=/log/pseidon/test/dt=20160320/hr=2016032009/Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz	perm=gvanvuuren:supergroup:rw-r--r--	proto=rpc
 INFO main pseidon-hdfs.hdfs-copy-service - Deleting file  target/test/copyservicetest/1480463789819/test-2016-03-20-09.gz
 INFO IPC Server handler 8 on 54992 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/log/raw2/test/dt=20160320/hr=2016032009/Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz	dst=null	perm=null	proto=rpc

[31mFAIL[0m "Test copy with directory created" at (run-test690189439915084552.clj:152)
    Expected: true
      Actual: false
 INFO main org.apache.hadoop.hdfs.MiniDFSCluster - Shutting down DataNode 0
 INFO org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@79f59e0e org.apache.hadoop.hdfs.server.datanode.DataNode - Closing all peers.
 WARN main org.apache.hadoop.hdfs.server.datanode.DirectoryScanner - DirectoryScanner: shutdown has been called
 INFO VolumeScannerThread(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1) org.apache.hadoop.hdfs.server.datanode.VolumeScanner - VolumeScanner(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1, DS-95faf3fb-3733-49e6-817a-49683977e493) exiting.
 INFO VolumeScannerThread(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2) org.apache.hadoop.hdfs.server.datanode.VolumeScanner - VolumeScanner(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2, DS-3f394624-ac8a-4c16-a924-f3d2e7474855) exiting.
 INFO main org.mortbay.log - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
 INFO main org.apache.hadoop.ipc.Server - Stopping server on 54996
 INFO IPC Server listener on 54996 org.apache.hadoop.ipc.Server - Stopping IPC Server listener on 54996
 INFO IPC Server Responder org.apache.hadoop.ipc.Server - Stopping IPC Server Responder
 WARN DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.datanode.DataNode - BPOfferService for Block pool BP-12931932-10.0.0.10-1480463788100 (Datanode Uuid bc889bf9-a333-4d69-81ac-93b4c29fb564) service to localhost/127.0.0.1:54992 interrupted
 WARN DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.datanode.DataNode - Ending block pool service for: Block pool BP-12931932-10.0.0.10-1480463788100 (Datanode Uuid bc889bf9-a333-4d69-81ac-93b4c29fb564) service to localhost/127.0.0.1:54992
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.datanode.DataNode - Removed Block pool BP-12931932-10.0.0.10-1480463788100 (Datanode Uuid bc889bf9-a333-4d69-81ac-93b4c29fb564)
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54992 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Removing block pool BP-12931932-10.0.0.10-1480463788100
 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter@1317ecef org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - LazyWriter was interrupted, exiting
 INFO main org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService - Shutting down all async disk service threads
 INFO main org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService - All async disk service threads have been shut down
 INFO main org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService - Shutting down all async lazy persist service threads
 INFO main org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService - All async lazy persist service threads have been shut down
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - Shutdown complete.
Formatting using clusterid: testClusterID
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - No KeyProvider found.
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - fsLock is fair:true
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager - dfs.block.invalidate.limit=1000
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager - dfs.namenode.datanode.registration.ip-hostname-check=true
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - The block deletion will start around 2016 Nov 30 00:56:30
 INFO main org.apache.hadoop.util.GSet - Computing capacity for map BlocksMap
 INFO main org.apache.hadoop.util.GSet - VM type       = 64-bit
 INFO main org.apache.hadoop.util.GSet - 2.0% max memory 3.6 GB = 72.8 MB
 INFO main org.apache.hadoop.util.GSet - capacity      = 2^23 = 8388608 entries
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - dfs.block.access.token.enable=false
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - defaultReplication         = 1
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - maxReplication             = 512
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - minReplication             = 1
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - maxReplicationStreams      = 2
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - replicationRecheckInterval = 3000
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - encryptDataTransfer        = false
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - maxNumBlocksToLog          = 1000
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - fsOwner             = gvanvuuren (auth:SIMPLE)
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - supergroup          = supergroup
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - isPermissionEnabled = true
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - HA Enabled: false
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Append Enabled: true
 INFO main org.apache.hadoop.util.GSet - Computing capacity for map INodeMap
 INFO main org.apache.hadoop.util.GSet - VM type       = 64-bit
 INFO main org.apache.hadoop.util.GSet - 1.0% max memory 3.6 GB = 36.4 MB
 INFO main org.apache.hadoop.util.GSet - capacity      = 2^22 = 4194304 entries
 INFO main org.apache.hadoop.hdfs.server.namenode.NameNode - Caching file names occuring more than 10 times
 INFO main org.apache.hadoop.util.GSet - Computing capacity for map cachedBlocks
 INFO main org.apache.hadoop.util.GSet - VM type       = 64-bit
 INFO main org.apache.hadoop.util.GSet - 0.25% max memory 3.6 GB = 9.1 MB
 INFO main org.apache.hadoop.util.GSet - capacity      = 2^20 = 1048576 entries
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - dfs.namenode.safemode.min.datanodes = 0
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - dfs.namenode.safemode.extension     = 0
 INFO main org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics - NNTop conf: dfs.namenode.top.window.num.buckets = 10
 INFO main org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics - NNTop conf: dfs.namenode.top.num.users = 10
 INFO main org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Retry cache on namenode is enabled
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
 INFO main org.apache.hadoop.util.GSet - Computing capacity for map NameNodeRetryCache
 INFO main org.apache.hadoop.util.GSet - VM type       = 64-bit
 INFO main org.apache.hadoop.util.GSet - 0.029999999329447746% max memory 3.6 GB = 1.1 MB
 INFO main org.apache.hadoop.util.GSet - capacity      = 2^17 = 131072 entries
 INFO main org.apache.hadoop.hdfs.server.namenode.NNConf - ACLs enabled? false
 INFO main org.apache.hadoop.hdfs.server.namenode.NNConf - XAttrs enabled? true
 INFO main org.apache.hadoop.hdfs.server.namenode.NNConf - Maximum size of an xattr: 16384
 INFO main org.apache.hadoop.hdfs.server.namenode.FSImage - Allocated new BlockPoolId: BP-95267058-10.0.0.10-1480463791000
 INFO main org.apache.hadoop.hdfs.server.common.Storage - Storage directory /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/name1 has been successfully formatted.
 INFO main org.apache.hadoop.hdfs.server.common.Storage - Storage directory /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/name2 has been successfully formatted.
 INFO main org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager - Going to retain 1 images with txid >= 0
 INFO main org.apache.hadoop.hdfs.server.namenode.NameNode - createNameNode []
 INFO main org.apache.hadoop.metrics2.impl.MetricsSystemImpl - NameNode metrics system started (again)
 INFO main org.apache.hadoop.hdfs.server.namenode.NameNode - fs.defaultFS is hdfs://127.0.0.1:0
 INFO org.apache.hadoop.util.JvmPauseMonitor$Monitor@3e7fc07e org.apache.hadoop.util.JvmPauseMonitor - Starting JVM pause monitor
 INFO main org.apache.hadoop.hdfs.DFSUtil - Starting Web-server for hdfs at: http://localhost:0
 INFO main org.apache.hadoop.security.authentication.server.AuthenticationFilter - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
 INFO main org.apache.hadoop.http.HttpRequestLog - Http request log for http.requests.namenode is not defined
 INFO main org.apache.hadoop.http.HttpServer2 - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
 INFO main org.apache.hadoop.http.HttpServer2 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
 INFO main org.apache.hadoop.http.HttpServer2 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
 INFO main org.apache.hadoop.http.HttpServer2 - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
 INFO main org.apache.hadoop.http.HttpServer2 - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
 INFO main org.apache.hadoop.http.HttpServer2 - Jetty bound to port 55001
 INFO main org.mortbay.log - jetty-6.1.26.cloudera.4
 INFO main org.mortbay.log - Extract jar:file:/Users/gvanvuuren/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar!/webapps/hdfs to /var/folders/tk/w8__4rl57830l94b5y2l7g6w0000gn/T/Jetty_localhost_55001_hdfs____khf86z/webapp
 INFO main org.mortbay.log - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:55001
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - No KeyProvider found.
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - fsLock is fair:true
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager - dfs.block.invalidate.limit=1000
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager - dfs.namenode.datanode.registration.ip-hostname-check=true
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - The block deletion will start around 2016 Nov 30 00:56:31
 INFO main org.apache.hadoop.util.GSet - Computing capacity for map BlocksMap
 INFO main org.apache.hadoop.util.GSet - VM type       = 64-bit
 INFO main org.apache.hadoop.util.GSet - 2.0% max memory 3.6 GB = 72.8 MB
 INFO main org.apache.hadoop.util.GSet - capacity      = 2^23 = 8388608 entries
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - dfs.block.access.token.enable=false
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - defaultReplication         = 1
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - maxReplication             = 512
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - minReplication             = 1
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - maxReplicationStreams      = 2
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - replicationRecheckInterval = 3000
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - encryptDataTransfer        = false
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - maxNumBlocksToLog          = 1000
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - fsOwner             = gvanvuuren (auth:SIMPLE)
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - supergroup          = supergroup
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - isPermissionEnabled = true
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - HA Enabled: false
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Append Enabled: true
 INFO main org.apache.hadoop.util.GSet - Computing capacity for map INodeMap
 INFO main org.apache.hadoop.util.GSet - VM type       = 64-bit
 INFO main org.apache.hadoop.util.GSet - 1.0% max memory 3.6 GB = 36.4 MB
 INFO main org.apache.hadoop.util.GSet - capacity      = 2^22 = 4194304 entries
 INFO main org.apache.hadoop.hdfs.server.namenode.NameNode - Caching file names occuring more than 10 times
 INFO main org.apache.hadoop.util.GSet - Computing capacity for map cachedBlocks
 INFO main org.apache.hadoop.util.GSet - VM type       = 64-bit
 INFO main org.apache.hadoop.util.GSet - 0.25% max memory 3.6 GB = 9.1 MB
 INFO main org.apache.hadoop.util.GSet - capacity      = 2^20 = 1048576 entries
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - dfs.namenode.safemode.min.datanodes = 0
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - dfs.namenode.safemode.extension     = 0
 INFO main org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics - NNTop conf: dfs.namenode.top.window.num.buckets = 10
 INFO main org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics - NNTop conf: dfs.namenode.top.num.users = 10
 INFO main org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Retry cache on namenode is enabled
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
 INFO main org.apache.hadoop.util.GSet - Computing capacity for map NameNodeRetryCache
 INFO main org.apache.hadoop.util.GSet - VM type       = 64-bit
 INFO main org.apache.hadoop.util.GSet - 0.029999999329447746% max memory 3.6 GB = 1.1 MB
 INFO main org.apache.hadoop.util.GSet - capacity      = 2^17 = 131072 entries
 INFO main org.apache.hadoop.hdfs.server.namenode.NNConf - ACLs enabled? false
 INFO main org.apache.hadoop.hdfs.server.namenode.NNConf - XAttrs enabled? true
 INFO main org.apache.hadoop.hdfs.server.namenode.NNConf - Maximum size of an xattr: 16384
 INFO main org.apache.hadoop.hdfs.server.common.Storage - Lock on /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/name1/in_use.lock acquired by nodename 9517@Gerrrits-MacBook-Pro.local
 INFO main org.apache.hadoop.hdfs.server.common.Storage - Lock on /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/name2/in_use.lock acquired by nodename 9517@Gerrrits-MacBook-Pro.local
 INFO main org.apache.hadoop.hdfs.server.namenode.FileJournalManager - Recovering unfinalized segments in /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/name1/current
 INFO main org.apache.hadoop.hdfs.server.namenode.FileJournalManager - Recovering unfinalized segments in /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/name2/current
 INFO main org.apache.hadoop.hdfs.server.namenode.FSImage - No edit log streams selected.
 INFO main org.apache.hadoop.hdfs.server.namenode.FSImage - Planning to load image: FSImageFile(file=/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/name1/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
 INFO main org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode - Loading 1 INodes.
 INFO main org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf - Loaded FSImage in 0 seconds.
 INFO main org.apache.hadoop.hdfs.server.namenode.FSImage - Loaded image for txid 0 from /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/name1/current/fsimage_0000000000000000000
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
 INFO main org.apache.hadoop.hdfs.server.namenode.FSEditLog - Starting log segment at 1
 INFO main org.apache.hadoop.hdfs.server.namenode.NameCache - initialized with 0 entries 0 lookups
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Finished loading FSImage in 11 msecs
 INFO main org.apache.hadoop.hdfs.server.namenode.NameNode - RPC server is binding to localhost:0
 INFO main org.apache.hadoop.ipc.CallQueueManager - Using callQueue class java.util.concurrent.LinkedBlockingQueue
 INFO Socket Reader #1 for port 55002 org.apache.hadoop.ipc.Server - Starting Socket Reader #1 for port 55002
 INFO main org.apache.hadoop.hdfs.server.namenode.NameNode - Clients are to use localhost:55002 to access this namenode/service.
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Registered FSNamesystemState MBean
 INFO main org.apache.hadoop.hdfs.server.namenode.LeaseManager - Number of blocks under construction: 0
 INFO main org.apache.hadoop.hdfs.server.namenode.LeaseManager - Number of blocks under construction: 0
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - initializing replication queues
 INFO main org.apache.hadoop.hdfs.StateChange - STATE* Leaving safe mode after 0 secs
 INFO main org.apache.hadoop.hdfs.StateChange - STATE* Network topology has 0 racks and 0 datanodes
 INFO main org.apache.hadoop.hdfs.StateChange - STATE* UnderReplicatedBlocks has 0 blocks
 INFO Replication Queue Initializer org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - Total number of blocks            = 0
 INFO Replication Queue Initializer org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - Number of invalid blocks          = 0
 INFO Replication Queue Initializer org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - Number of under-replicated blocks = 0
 INFO Replication Queue Initializer org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - Number of  over-replicated blocks = 0
 INFO Replication Queue Initializer org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - Number of blocks being written    = 0
 INFO Replication Queue Initializer org.apache.hadoop.hdfs.StateChange - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 5 msec
 INFO IPC Server Responder org.apache.hadoop.ipc.Server - IPC Server Responder: starting
 INFO IPC Server listener on 55002 org.apache.hadoop.ipc.Server - IPC Server listener on 55002: starting
 INFO main org.apache.hadoop.hdfs.server.namenode.NameNode - NameNode RPC up at: localhost/127.0.0.1:55002
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Starting services required for active state
 INFO CacheReplicationMonitor(2058852053) org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor - Starting CacheReplicationMonitor with interval 30000 milliseconds
 INFO CacheReplicationMonitor(2058852053) org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor - Rescanning after 137601884 milliseconds
 INFO CacheReplicationMonitor(2058852053) org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor - Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
 INFO main org.apache.hadoop.hdfs.MiniDFSCluster - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1,[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2
 INFO main org.apache.hadoop.metrics2.impl.MetricsSystemImpl - DataNode metrics system started (again)
 INFO main org.apache.hadoop.hdfs.server.datanode.BlockScanner - Initialized block scanner with targetBytesPerSec 1048576
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - Configured hostname is 127.0.0.1
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - Starting DataNode with maxLockedMemory = 0
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - Opened streaming server at /127.0.0.1:55003
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - Balancing bandwith is 1048576 bytes/s
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - Number threads for balancing is 5
 INFO main org.apache.hadoop.security.authentication.server.AuthenticationFilter - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
 INFO main org.apache.hadoop.http.HttpRequestLog - Http request log for http.requests.datanode is not defined
 INFO main org.apache.hadoop.http.HttpServer2 - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
 INFO main org.apache.hadoop.http.HttpServer2 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
 INFO main org.apache.hadoop.http.HttpServer2 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
 INFO main org.apache.hadoop.http.HttpServer2 - Jetty bound to port 55004
 INFO main org.mortbay.log - jetty-6.1.26.cloudera.4
 INFO main org.mortbay.log - Extract jar:file:/Users/gvanvuuren/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar!/webapps/datanode to /var/folders/tk/w8__4rl57830l94b5y2l7g6w0000gn/T/Jetty_localhost_55004_datanode____.q3s3gl/webapp
 INFO main org.mortbay.log - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:55004
 INFO main org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer - Listening HTTP traffic on /127.0.0.1:55005
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - dnUserName = gvanvuuren
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - supergroup = supergroup
 INFO org.apache.hadoop.util.JvmPauseMonitor$Monitor@25630ede org.apache.hadoop.util.JvmPauseMonitor - Starting JVM pause monitor
 INFO main org.apache.hadoop.ipc.CallQueueManager - Using callQueue class java.util.concurrent.LinkedBlockingQueue
 INFO Socket Reader #1 for port 55006 org.apache.hadoop.ipc.Server - Starting Socket Reader #1 for port 55006
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - Opened IPC server at /127.0.0.1:55006
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - Refresh request received for nameservices: null
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - Starting BPOfferServices for nameservices: <default>
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.datanode.DataNode - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:55002 starting to offer service
 INFO IPC Server Responder org.apache.hadoop.ipc.Server - IPC Server Responder: starting
 INFO IPC Server listener on 55006 org.apache.hadoop.ipc.Server - IPC Server listener on 55006: starting
 INFO main org.apache.hadoop.hdfs.MiniDFSCluster - dnInfo.length != numDataNodes
 INFO main org.apache.hadoop.hdfs.MiniDFSCluster - Waiting for cluster to become active
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.common.Storage - Lock on /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/in_use.lock acquired by nodename 9517@Gerrrits-MacBook-Pro.local
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.common.Storage - Storage directory /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1 is not formatted for BP-95267058-10.0.0.10-1480463791000
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.common.Storage - Formatting ...
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.common.Storage - Analyzing storage directories for bpid BP-95267058-10.0.0.10-1480463791000
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.common.Storage - Locking is disabled for /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/current/BP-95267058-10.0.0.10-1480463791000
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.common.Storage - Block pool storage directory /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/current/BP-95267058-10.0.0.10-1480463791000 is not formatted for BP-95267058-10.0.0.10-1480463791000
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.common.Storage - Formatting ...
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.common.Storage - Formatting block pool BP-95267058-10.0.0.10-1480463791000 directory /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/current/BP-95267058-10.0.0.10-1480463791000/current
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.common.Storage - Lock on /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/in_use.lock acquired by nodename 9517@Gerrrits-MacBook-Pro.local
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.common.Storage - Storage directory /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2 is not formatted for BP-95267058-10.0.0.10-1480463791000
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.common.Storage - Formatting ...
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.common.Storage - Analyzing storage directories for bpid BP-95267058-10.0.0.10-1480463791000
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.common.Storage - Locking is disabled for /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/current/BP-95267058-10.0.0.10-1480463791000
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.common.Storage - Block pool storage directory /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/current/BP-95267058-10.0.0.10-1480463791000 is not formatted for BP-95267058-10.0.0.10-1480463791000
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.common.Storage - Formatting ...
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.common.Storage - Formatting block pool BP-95267058-10.0.0.10-1480463791000 directory /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/current/BP-95267058-10.0.0.10-1480463791000/current
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.datanode.DataNode - Setting up storage: nsid=662172658;bpid=BP-95267058-10.0.0.10-1480463791000;lv=-56;nsInfo=lv=-60;cid=testClusterID;nsid=662172658;c=0;bpid=BP-95267058-10.0.0.10-1480463791000;dnuuid=null
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.datanode.DataNode - Generated and persisted new Datanode UUID b0fb9d69-d3a3-4572-a940-d1eea5cfab14
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Added new volume: DS-6bfacfee-5d2e-4711-b043-433c55513c7d
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Added volume - /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/current, StorageType: DISK
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Added new volume: DS-4c5ee6b3-d96a-4b18-a1f4-4f2bc18c3ffe
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Added volume - /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/current, StorageType: DISK
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Registered FSDatasetState MBean
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Volume reference is released.
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Adding block pool BP-95267058-10.0.0.10-1480463791000
 INFO Thread-148 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Scanning block pool BP-95267058-10.0.0.10-1480463791000 on volume /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/current...
 INFO Thread-149 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Scanning block pool BP-95267058-10.0.0.10-1480463791000 on volume /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/current...
 INFO Thread-149 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Time taken to scan block pool BP-95267058-10.0.0.10-1480463791000 on /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/current: 9ms
 INFO Thread-148 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Time taken to scan block pool BP-95267058-10.0.0.10-1480463791000 on /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/current: 11ms
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Total time to scan all replicas for block pool BP-95267058-10.0.0.10-1480463791000: 11ms
 INFO Thread-152 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Adding replicas to map for block pool BP-95267058-10.0.0.10-1480463791000 on volume /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/current...
 INFO Thread-153 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Adding replicas to map for block pool BP-95267058-10.0.0.10-1480463791000 on volume /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/current...
 INFO Thread-152 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Time to add replicas to map for block pool BP-95267058-10.0.0.10-1480463791000 on volume /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/current: 0ms
 INFO Thread-153 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Time to add replicas to map for block pool BP-95267058-10.0.0.10-1480463791000 on volume /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/current: 0ms
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Total time to add all replicas to map: 0ms
 INFO VolumeScannerThread(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2) org.apache.hadoop.hdfs.server.datanode.VolumeScanner - Now scanning bpid BP-95267058-10.0.0.10-1480463791000 on volume /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2
 INFO VolumeScannerThread(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1) org.apache.hadoop.hdfs.server.datanode.VolumeScanner - Now scanning bpid BP-95267058-10.0.0.10-1480463791000 on volume /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.datanode.DirectoryScanner - Periodic Directory Tree Verification scan starting at 1480481003424ms with interval of 21600000ms
 INFO VolumeScannerThread(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2) org.apache.hadoop.hdfs.server.datanode.VolumeScanner - VolumeScanner(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2, DS-4c5ee6b3-d96a-4b18-a1f4-4f2bc18c3ffe): finished scanning block pool BP-95267058-10.0.0.10-1480463791000
 INFO VolumeScannerThread(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1) org.apache.hadoop.hdfs.server.datanode.VolumeScanner - VolumeScanner(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1, DS-6bfacfee-5d2e-4711-b043-433c55513c7d): finished scanning block pool BP-95267058-10.0.0.10-1480463791000
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.datanode.DataNode - Block pool BP-95267058-10.0.0.10-1480463791000 (Datanode Uuid null) service to localhost/127.0.0.1:55002 beginning handshake with NN
 INFO VolumeScannerThread(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1) org.apache.hadoop.hdfs.server.datanode.VolumeScanner - VolumeScanner(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1, DS-6bfacfee-5d2e-4711-b043-433c55513c7d): no suitable block pools found to scan.  Waiting 1814399998 ms.
 INFO VolumeScannerThread(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2) org.apache.hadoop.hdfs.server.datanode.VolumeScanner - VolumeScanner(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2, DS-4c5ee6b3-d96a-4b18-a1f4-4f2bc18c3ffe): no suitable block pools found to scan.  Waiting 1814399998 ms.
 INFO IPC Server handler 2 on 55002 org.apache.hadoop.hdfs.StateChange - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=b0fb9d69-d3a3-4572-a940-d1eea5cfab14, infoPort=55005, infoSecurePort=0, ipcPort=55006, storageInfo=lv=-56;cid=testClusterID;nsid=662172658;c=0) storage b0fb9d69-d3a3-4572-a940-d1eea5cfab14
 INFO IPC Server handler 2 on 55002 org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor - Number of failed storage changes from 0 to 0
 INFO IPC Server handler 2 on 55002 org.apache.hadoop.net.NetworkTopology - Adding a new node: /default-rack/127.0.0.1:55003
 INFO IPC Server handler 2 on 55002 org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager - Registered DN b0fb9d69-d3a3-4572-a940-d1eea5cfab14 (127.0.0.1:55003).
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.datanode.DataNode - Block pool Block pool BP-95267058-10.0.0.10-1480463791000 (Datanode Uuid null) service to localhost/127.0.0.1:55002 successfully registered with NN
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.datanode.DataNode - For namenode localhost/127.0.0.1:55002 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
 INFO IPC Server handler 3 on 55002 org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor - Number of failed storage changes from 0 to 0
 INFO IPC Server handler 3 on 55002 org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor - Adding new storage ID DS-6bfacfee-5d2e-4711-b043-433c55513c7d for DN 127.0.0.1:55003
 INFO IPC Server handler 3 on 55002 org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor - Adding new storage ID DS-4c5ee6b3-d96a-4b18-a1f4-4f2bc18c3ffe for DN 127.0.0.1:55003
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.datanode.DataNode - Namenode Block pool BP-95267058-10.0.0.10-1480463791000 (Datanode Uuid b0fb9d69-d3a3-4572-a940-d1eea5cfab14) service to localhost/127.0.0.1:55002 trying to claim ACTIVE state with txid=1
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.datanode.DataNode - Acknowledging ACTIVE Namenode Block pool BP-95267058-10.0.0.10-1480463791000 (Datanode Uuid b0fb9d69-d3a3-4572-a940-d1eea5cfab14) service to localhost/127.0.0.1:55002
 INFO IPC Server handler 4 on 55002 org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - Processing first storage report for DS-4c5ee6b3-d96a-4b18-a1f4-4f2bc18c3ffe from datanode b0fb9d69-d3a3-4572-a940-d1eea5cfab14
 INFO IPC Server handler 4 on 55002 BlockStateChange - BLOCK* processReport: from storage DS-4c5ee6b3-d96a-4b18-a1f4-4f2bc18c3ffe node DatanodeRegistration(127.0.0.1, datanodeUuid=b0fb9d69-d3a3-4572-a940-d1eea5cfab14, infoPort=55005, infoSecurePort=0, ipcPort=55006, storageInfo=lv=-56;cid=testClusterID;nsid=662172658;c=0), blocks: 0, hasStaleStorage: true, processing time: 1 msecs
 INFO IPC Server handler 4 on 55002 org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - Processing first storage report for DS-6bfacfee-5d2e-4711-b043-433c55513c7d from datanode b0fb9d69-d3a3-4572-a940-d1eea5cfab14
 INFO IPC Server handler 4 on 55002 BlockStateChange - BLOCK* processReport: from storage DS-6bfacfee-5d2e-4711-b043-433c55513c7d node DatanodeRegistration(127.0.0.1, datanodeUuid=b0fb9d69-d3a3-4572-a940-d1eea5cfab14, infoPort=55005, infoSecurePort=0, ipcPort=55006, storageInfo=lv=-56;cid=testClusterID;nsid=662172658;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.datanode.DataNode - Successfully sent block report 0x1692480876d812fe,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.datanode.DataNode - Got finalize command for block pool BP-95267058-10.0.0.10-1480463791000
 INFO main org.apache.hadoop.hdfs.MiniDFSCluster - Cluster is active
 INFO main org.apache.hadoop.hdfs.MiniDFSCluster - Cluster is active
 ERROR main pseidon-hdfs.hdfs-copy-service - copying corrupt file  target/test/copyservicetest/1480463791468/test-2016-03-20-09.gz   to quarantine  /tmp/pseidon-quarantine/test/dt=20160320/hr=2016032009/Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz
 INFO IPC Server handler 7 on 55002 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tmp/pseidon-quarantine/test/dt=20160320/hr=2016032009	dst=null	perm=null	proto=rpc
 INFO main pseidon-hdfs.hdfs-util - creating hdfs dir  /tmp/pseidon-quarantine/test/dt=20160320/hr=2016032009
 INFO IPC Server handler 8 on 55002 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/tmp/pseidon-quarantine/test/dt=20160320/hr=2016032009	dst=null	perm=gvanvuuren:supergroup:rwxr-xr-x	proto=rpc
 INFO main pseidon-hdfs.hdfs-util - calling on-create
 INFO IPC Server handler 9 on 55002 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tmp/copying_tmp_pseidon-quarantine_test_dt=20160320_hr=2016032009_Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz	dst=null	perm=null	proto=rpc
 INFO IPC Server handler 0 on 55002 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/tmp/copying_tmp_pseidon-quarantine_test_dt=20160320_hr=2016032009_Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz	dst=null	perm=gvanvuuren:supergroup:rw-r--r--	proto=rpc
 INFO IPC Server handler 1 on 55002 org.apache.hadoop.hdfs.StateChange - BLOCK* allocateBlock: /tmp/copying_tmp_pseidon-quarantine_test_dt=20160320_hr=2016032009_Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz. BP-95267058-10.0.0.10-1480463791000 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-6bfacfee-5d2e-4711-b043-433c55513c7d:NORMAL:127.0.0.1:55003|RBW]]}
 INFO DataXceiver for client DFSClient_NONMAPREDUCE_-463833773_1 at /127.0.0.1:55009 [Receiving block BP-95267058-10.0.0.10-1480463791000:blk_1073741825_1001] org.apache.hadoop.hdfs.server.datanode.DataNode - Receiving BP-95267058-10.0.0.10-1480463791000:blk_1073741825_1001 src: /127.0.0.1:55009 dest: /127.0.0.1:55003
 INFO PacketResponder: BP-95267058-10.0.0.10-1480463791000:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace - src: /127.0.0.1:55009, dest: /127.0.0.1:55003, bytes: 7, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-463833773_1, offset: 0, srvID: b0fb9d69-d3a3-4572-a940-d1eea5cfab14, blockid: BP-95267058-10.0.0.10-1480463791000:blk_1073741825_1001, duration: 1730595
 INFO PacketResponder: BP-95267058-10.0.0.10-1480463791000:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] org.apache.hadoop.hdfs.server.datanode.DataNode - PacketResponder: BP-95267058-10.0.0.10-1480463791000:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
 INFO IPC Server handler 3 on 55002 BlockStateChange - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:55003 is added to blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-6bfacfee-5d2e-4711-b043-433c55513c7d:NORMAL:127.0.0.1:55003|RBW]]} size 0
 INFO IPC Server handler 4 on 55002 org.apache.hadoop.hdfs.StateChange - DIR* completeFile: /tmp/copying_tmp_pseidon-quarantine_test_dt=20160320_hr=2016032009_Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz is closed by DFSClient_NONMAPREDUCE_-463833773_1
 INFO IPC Server handler 5 on 55002 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=rename	src=/tmp/copying_tmp_pseidon-quarantine_test_dt=20160320_hr=2016032009_Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz	dst=/tmp/pseidon-quarantine/test/dt=20160320/hr=2016032009/Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz	perm=gvanvuuren:supergroup:rw-r--r--	proto=rpc
 INFO main pseidon-hdfs.hdfs-copy-service - Deleting file  target/test/copyservicetest/1480463791468/test-2016-03-20-09.gz
 ERROR main pseidon-hdfs.hdfs-copy-service - copying corrupt file  target/test/copyservicetest/1480463791468/test-2016-03-20-09.gz   to quarantine  /tmp/pseidon-quarantine/test/dt=20160320/hr=2016032009/Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz
 INFO IPC Server handler 6 on 55002 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tmp/pseidon-quarantine/test/dt=20160320/hr=2016032009	dst=null	perm=null	proto=rpc
 INFO IPC Server handler 7 on 55002 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tmp/copying_tmp_pseidon-quarantine_test_dt=20160320_hr=2016032009_Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz	dst=null	perm=null	proto=rpc
 INFO IPC Server handler 8 on 55002 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/tmp/copying_tmp_pseidon-quarantine_test_dt=20160320_hr=2016032009_Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz	dst=null	perm=gvanvuuren:supergroup:rw-r--r--	proto=rpc
 INFO IPC Server handler 9 on 55002 org.apache.hadoop.hdfs.StateChange - BLOCK* allocateBlock: /tmp/copying_tmp_pseidon-quarantine_test_dt=20160320_hr=2016032009_Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz. BP-95267058-10.0.0.10-1480463791000 blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-6bfacfee-5d2e-4711-b043-433c55513c7d:NORMAL:127.0.0.1:55003|RBW]]}
 INFO DataXceiver for client DFSClient_NONMAPREDUCE_-463833773_1 at /127.0.0.1:55010 [Receiving block BP-95267058-10.0.0.10-1480463791000:blk_1073741826_1002] org.apache.hadoop.hdfs.server.datanode.DataNode - Receiving BP-95267058-10.0.0.10-1480463791000:blk_1073741826_1002 src: /127.0.0.1:55010 dest: /127.0.0.1:55003
 INFO PacketResponder: BP-95267058-10.0.0.10-1480463791000:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace - src: /127.0.0.1:55010, dest: /127.0.0.1:55003, bytes: 7, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-463833773_1, offset: 0, srvID: b0fb9d69-d3a3-4572-a940-d1eea5cfab14, blockid: BP-95267058-10.0.0.10-1480463791000:blk_1073741826_1002, duration: 2066429
 INFO PacketResponder: BP-95267058-10.0.0.10-1480463791000:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] org.apache.hadoop.hdfs.server.datanode.DataNode - PacketResponder: BP-95267058-10.0.0.10-1480463791000:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
 INFO IPC Server handler 0 on 55002 BlockStateChange - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:55003 is added to blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-4c5ee6b3-d96a-4b18-a1f4-4f2bc18c3ffe:NORMAL:127.0.0.1:55003|FINALIZED]]} size 0
 INFO IPC Server handler 1 on 55002 org.apache.hadoop.hdfs.StateChange - DIR* completeFile: /tmp/copying_tmp_pseidon-quarantine_test_dt=20160320_hr=2016032009_Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz is closed by DFSClient_NONMAPREDUCE_-463833773_1
 WARN IPC Server handler 2 on 55002 org.apache.hadoop.hdfs.StateChange - DIR* FSDirectory.unprotectedRenameTo: failed to rename /tmp/copying_tmp_pseidon-quarantine_test_dt=20160320_hr=2016032009_Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz to /tmp/pseidon-quarantine/test/dt=20160320/hr=2016032009/Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz because destination exists
 INFO IPC Server handler 2 on 55002 org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream - Nothing to flush
 INFO IPC Server handler 2 on 55002 org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream - Nothing to flush
 INFO IPC Server handler 3 on 55002 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tmp/pseidon-quarantine/test/dt=20160320/hr=2016032009/Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz	dst=null	perm=null	proto=rpc
 INFO IPC Server handler 4 on 55002 BlockStateChange - BLOCK* addToInvalidates: blk_1073741825_1001 127.0.0.1:55003 
 INFO IPC Server handler 4 on 55002 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=delete	src=/tmp/pseidon-quarantine/test/dt=20160320/hr=2016032009/Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz	dst=null	perm=null	proto=rpc
 INFO IPC Server handler 5 on 55002 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=rename	src=/tmp/copying_tmp_pseidon-quarantine_test_dt=20160320_hr=2016032009_Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz	dst=/tmp/pseidon-quarantine/test/dt=20160320/hr=2016032009/Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz	perm=gvanvuuren:supergroup:rw-r--r--	proto=rpc
 INFO main pseidon-hdfs.hdfs-copy-service - Deleting file  target/test/copyservicetest/1480463791468/test-2016-03-20-09.gz
 INFO IPC Server handler 6 on 55002 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tmp/pseidon-etl-quarantine/test/dt=20160320/hr=2016032009/Gerrrits-MacBook-Pro.local-test-2016-03-20-09.gz	dst=null	perm=null	proto=rpc

[31mFAIL[0m "Test copy with corrupt files are sent to quarantine" at (run-test690189439915084552.clj:157)
    Expected: true
      Actual: false
 INFO main org.apache.hadoop.hdfs.MiniDFSCluster - Shutting down DataNode 0
 INFO org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@34893693 org.apache.hadoop.hdfs.server.datanode.DataNode - Closing all peers.
 WARN main org.apache.hadoop.hdfs.server.datanode.DirectoryScanner - DirectoryScanner: shutdown has been called
 INFO VolumeScannerThread(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2) org.apache.hadoop.hdfs.server.datanode.VolumeScanner - VolumeScanner(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2, DS-4c5ee6b3-d96a-4b18-a1f4-4f2bc18c3ffe) exiting.
 INFO VolumeScannerThread(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1) org.apache.hadoop.hdfs.server.datanode.VolumeScanner - VolumeScanner(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1, DS-6bfacfee-5d2e-4711-b043-433c55513c7d) exiting.
 INFO main org.mortbay.log - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
 INFO main org.apache.hadoop.ipc.Server - Stopping server on 55006
 INFO IPC Server listener on 55006 org.apache.hadoop.ipc.Server - Stopping IPC Server listener on 55006
 INFO IPC Server Responder org.apache.hadoop.ipc.Server - Stopping IPC Server Responder
 WARN DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.datanode.DataNode - BPOfferService for Block pool BP-95267058-10.0.0.10-1480463791000 (Datanode Uuid b0fb9d69-d3a3-4572-a940-d1eea5cfab14) service to localhost/127.0.0.1:55002 interrupted
 WARN DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.datanode.DataNode - Ending block pool service for: Block pool BP-95267058-10.0.0.10-1480463791000 (Datanode Uuid b0fb9d69-d3a3-4572-a940-d1eea5cfab14) service to localhost/127.0.0.1:55002
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.datanode.DataNode - Removed Block pool BP-95267058-10.0.0.10-1480463791000 (Datanode Uuid b0fb9d69-d3a3-4572-a940-d1eea5cfab14)
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55002 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Removing block pool BP-95267058-10.0.0.10-1480463791000
 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter@596650da org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - LazyWriter was interrupted, exiting
 INFO main org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService - Shutting down all async disk service threads
 INFO main org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService - All async disk service threads have been shut down
 INFO main org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService - Shutting down all async lazy persist service threads
 INFO main org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService - All async lazy persist service threads have been shut down
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - Shutdown complete.

Ran 3 tests containing 2 assertions.
2 failures, 0 errors.

Ran 2 tests containing 3 assertions.
0 failures, 0 errors.
 INFO main org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
 INFO main org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
 INFO main DataNucleus.Persistence - Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
 INFO main DataNucleus.Persistence - Property datanucleus.cache.level2 unknown - will be ignored
 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@63d9e1a BlockStateChange - BLOCK* BlockManager: ask 127.0.0.1:54993 to delete [blk_1073741825_1001]
 INFO main org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
 INFO main DataNucleus.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
 INFO main DataNucleus.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@5694f6a0 BlockStateChange - BLOCK* BlockManager: ask 127.0.0.1:55003 to delete [blk_1073741825_1001]
 INFO main DataNucleus.Datastore - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
 INFO main DataNucleus.Datastore - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
 INFO main DataNucleus.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
 INFO main org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is DERBY
 INFO main org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
 INFO main org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
 INFO main org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
 INFO main org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
 INFO main org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=get_all_functions from=org.apache.hadoop.hive.metastore.RetryingHMSHandler>
 INFO main org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_functions
 INFO main org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=gvanvuuren	ip=unknown-ip-addr	cmd=get_all_functions	
 INFO main DataNucleus.Datastore - The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
 INFO main org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=get_all_functions start=1480463794756 end=1480463794799 duration=43 from=org.apache.hadoop.hive.metastore.RetryingHMSHandler threadId=0 retryCount=0 error=false>
 INFO main org.apache.hadoop.hive.ql.session.SessionState - Created local directory: /var/folders/tk/w8__4rl57830l94b5y2l7g6w0000gn/T/0bfcda6b-371a-48de-b135-9ba7cec41ef6_resources
 INFO main org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/gvanvuuren/0bfcda6b-371a-48de-b135-9ba7cec41ef6
 INFO main org.apache.hadoop.hive.ql.session.SessionState - Created local directory: /var/folders/tk/w8__4rl57830l94b5y2l7g6w0000gn/T/gvanvuuren/0bfcda6b-371a-48de-b135-9ba7cec41ef6
 INFO main org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/gvanvuuren/0bfcda6b-371a-48de-b135-9ba7cec41ef6/_tmp_space.db
 INFO main org.apache.hadoop.hive.ql.session.SessionState - No Tez session required at this point. hive.execution.engine=mr.
 INFO main org.apache.hive.service.CompositeService - Operation log root directory is created: /var/folders/tk/w8__4rl57830l94b5y2l7g6w0000gn/T/gvanvuuren/operation_logs
 INFO main org.apache.hive.service.CompositeService - HiveServer2: Background operation thread pool size: 100
 INFO main org.apache.hive.service.CompositeService - HiveServer2: Background operation thread wait queue size: 100
 INFO main org.apache.hive.service.CompositeService - HiveServer2: Background operation thread keepalive time: 10 seconds
 INFO main org.apache.hive.service.AbstractService - Service:OperationManager is inited.
 INFO main org.apache.hive.service.AbstractService - Service:SessionManager is inited.
 INFO main org.apache.hive.service.AbstractService - Service:CLIService is inited.
 INFO main org.apache.hive.service.AbstractService - Service:ThriftBinaryCLIService is inited.
 INFO main org.apache.hive.service.AbstractService - Service:HiveServer2 is inited.
 INFO main org.apache.hive.service.AbstractService - Service:OperationManager is started.
 INFO main org.apache.hive.service.AbstractService - Service:SessionManager is started.
 INFO main org.apache.hive.service.AbstractService - Service:CLIService is started.
 INFO main org.apache.hive.service.AbstractService - Service:ThriftBinaryCLIService is started.
 INFO main org.apache.hive.service.AbstractService - Service:HiveServer2 is started.
 INFO main org.eclipse.jetty.server.Server - jetty-7.6.0.v20120127
 INFO main org.eclipse.jetty.webapp.WebInfConfiguration - Extract jar:file:/Users/gvanvuuren/.m2/repository/org/apache/hive/hive-service/1.1.0-cdh5.7.0/hive-service-1.1.0-cdh5.7.0.jar!/hive-webapps/hiveserver2/ to /private/var/folders/tk/w8__4rl57830l94b5y2l7g6w0000gn/T/jetty-0.0.0.0-10002-hiveserver2-_-any-/webapp
 INFO main org.eclipse.jetty.server.handler.ContextHandler - started o.e.j.w.WebAppContext{/,file:/private/var/folders/tk/w8__4rl57830l94b5y2l7g6w0000gn/T/jetty-0.0.0.0-10002-hiveserver2-_-any-/webapp/},jar:file:/Users/gvanvuuren/.m2/repository/org/apache/hive/hive-service/1.1.0-cdh5.7.0/hive-service-1.1.0-cdh5.7.0.jar!/hive-webapps/hiveserver2
 INFO main org.eclipse.jetty.server.handler.ContextHandler - started o.e.j.s.ServletContextHandler{/static,jar:file:/Users/gvanvuuren/.m2/repository/org/apache/hive/hive-service/1.1.0-cdh5.7.0/hive-service-1.1.0-cdh5.7.0.jar!/hive-webapps/static}
 INFO main org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:10002
 INFO main org.apache.hive.service.server.HiveServer2 - Web UI has started on port 10002
 INFO main pseidon-hdfs.hive - Creating hive connection:  jdbc:hive2://localhost:10000 hive 
 INFO main org.apache.hive.jdbc.Utils - Supplied authorities: localhost:10000
 INFO main org.apache.hive.jdbc.Utils - Resolved authority: localhost:10000
 INFO HiveServer2-Handler-Pool: Thread-291 org.apache.hive.service.cli.thrift.ThriftCLIService - Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V7
 INFO HiveServer2-Handler-Pool: Thread-291 org.apache.hadoop.hive.ql.session.SessionState - Created local directory: /var/folders/tk/w8__4rl57830l94b5y2l7g6w0000gn/T/7f2d58c1-ea90-4ed1-b61d-9c5e3230ff7a_resources
 INFO HiveServer2-Handler-Pool: Thread-291 org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/hive/7f2d58c1-ea90-4ed1-b61d-9c5e3230ff7a
 INFO HiveServer2-Handler-Pool: Thread-291 org.apache.hadoop.hive.ql.session.SessionState - Created local directory: /var/folders/tk/w8__4rl57830l94b5y2l7g6w0000gn/T/gvanvuuren/7f2d58c1-ea90-4ed1-b61d-9c5e3230ff7a
 INFO HiveServer2-Handler-Pool: Thread-291 org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/hive/7f2d58c1-ea90-4ed1-b61d-9c5e3230ff7a/_tmp_space.db
 INFO HiveServer2-Handler-Pool: Thread-291 org.apache.hadoop.hive.ql.session.SessionState - No Tez session required at this point. hive.execution.engine=mr.
 INFO HiveServer2-Handler-Pool: Thread-291 org.apache.hive.service.cli.session.HiveSessionImpl - Operation log session directory is created: /var/folders/tk/w8__4rl57830l94b5y2l7g6w0000gn/T/gvanvuuren/operation_logs/7f2d58c1-ea90-4ed1-b61d-9c5e3230ff7a
 INFO HiveServer2-Handler-Pool: Thread-291 org.apache.hive.service.cli.thrift.ThriftCLIService - Opened a session, current sessions: 1
 INFO HiveServer2-Handler-Pool: Thread-291 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-291 org.apache.hadoop.hive.ql.Driver - Compiling command(queryId=gvanvuuren_20161130005656_d9e6b0f2-51a1-48e4-bd79-6cf6a313ff4a): CREATE DATABASE IF NOT EXISTS pseidon LOCATION '/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/target/pseidontest/1480463797386'
 INFO HiveServer2-Handler-Pool: Thread-291 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-291 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=parse start=1480463797435 end=1480463797851 duration=416 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-291 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-291 org.apache.hadoop.hive.ql.Driver - Semantic Analysis Completed
 INFO HiveServer2-Handler-Pool: Thread-291 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=semanticAnalyze start=1480463797853 end=1480463797899 duration=46 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-291 org.apache.hadoop.hive.ql.Driver - Returning Hive schema: Schema(fieldSchemas:null, properties:null)
 INFO HiveServer2-Handler-Pool: Thread-291 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=compile start=1480463797410 end=1480463797929 duration=519 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-291 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=compile start=1480463797410 end=1480463797929 duration=519 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-291 org.apache.hadoop.hive.ql.Driver - Completed compiling command(queryId=gvanvuuren_20161130005656_d9e6b0f2-51a1-48e4-bd79-6cf6a313ff4a); Time taken: 0.519 seconds
 INFO HiveServer2-Background-Pool: Thread-297 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-297 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-297 org.apache.hadoop.hive.ql.Driver - Concurrency mode is disabled, not creating a lock manager
 INFO HiveServer2-Background-Pool: Thread-297 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-297 org.apache.hadoop.hive.ql.Driver - Executing command(queryId=gvanvuuren_20161130005656_d9e6b0f2-51a1-48e4-bd79-6cf6a313ff4a): CREATE DATABASE IF NOT EXISTS pseidon LOCATION '/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/target/pseidontest/1480463797386'
 INFO HiveServer2-Background-Pool: Thread-297 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=TimeToSubmit start=1480463797934 end=1480463797939 duration=5 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-297 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-297 org.apache.hadoop.hive.ql.Driver - Starting task [Stage-0:DDL] in serial mode
 WARN HiveServer2-Background-Pool: Thread-297 org.apache.hadoop.security.ShellBasedUnixGroupsMapping - unable to return groups for user hive
PartialGroupNameException The user name 'hive' is not found. id: hive: no such user
id: hive: no such user

	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.resolvePartialGroupNames(ShellBasedUnixGroupsMapping.java:212)
	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:133)
	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:72)
	at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:51)
	at org.apache.hadoop.security.Groups$GroupCacheLoader.fetchGroupList(Groups.java:239)
	at org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:220)
	at org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:208)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.hadoop.security.Groups.getGroups(Groups.java:182)
	at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1553)
	at org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator.setConf(HadoopDefaultAuthenticator.java:64)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.ql.metadata.HiveUtils.getAuthenticator(HiveUtils.java:439)
	at org.apache.hadoop.hive.ql.session.SessionState.setupAuth(SessionState.java:729)
	at org.apache.hadoop.hive.ql.session.SessionState.getAuthenticator(SessionState.java:1270)
	at org.apache.hadoop.hive.ql.session.SessionState.getUserFromAuthenticator(SessionState.java:960)
	at org.apache.hadoop.hive.ql.exec.DDLTask.createDatabase(DDLTask.java:3790)
	at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:268)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1774)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1531)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1311)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1120)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1113)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:178)
	at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:72)
	at org.apache.hive.service.cli.operation.SQLOperation$2$1.run(SQLOperation.java:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
	at org.apache.hive.service.cli.operation.SQLOperation$2.run(SQLOperation.java:245)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
 WARN HiveServer2-Background-Pool: Thread-297 org.apache.hadoop.security.UserGroupInformation - No groups available for user hive
 WARN HiveServer2-Background-Pool: Thread-297 org.apache.hadoop.security.UserGroupInformation - No groups available for user hive
 INFO HiveServer2-Background-Pool: Thread-297 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=create_database from=org.apache.hadoop.hive.metastore.RetryingHMSHandler>
 INFO HiveServer2-Background-Pool: Thread-297 org.apache.hadoop.hive.metastore.HiveMetaStore - 1: create_database: Database(name:pseidon, description:null, locationUri:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/target/pseidontest/1480463797386, parameters:null, ownerName:hive, ownerType:USER)
 INFO HiveServer2-Background-Pool: Thread-297 org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=hive	ip=unknown-ip-addr	cmd=create_database: Database(name:pseidon, description:null, locationUri:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/target/pseidontest/1480463797386, parameters:null, ownerName:hive, ownerType:USER)	
 INFO HiveServer2-Background-Pool: Thread-297 org.apache.hadoop.hive.metastore.HiveMetaStore - 1: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
 INFO HiveServer2-Background-Pool: Thread-297 org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
 INFO HiveServer2-Background-Pool: Thread-297 DataNucleus.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
 INFO HiveServer2-Background-Pool: Thread-297 org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is DERBY
 INFO HiveServer2-Background-Pool: Thread-297 org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
 ERROR HiveServer2-Background-Pool: Thread-297 org.apache.hadoop.hive.metastore.RetryingHMSHandler - AlreadyExistsException(message:Database pseidon already exists)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:944)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:138)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)
	at com.sun.proxy.$Proxy26.create_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:646)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:105)
	at com.sun.proxy.$Proxy27.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:345)
	at org.apache.hadoop.hive.ql.exec.DDLTask.createDatabase(DDLTask.java:3796)
	at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:268)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1774)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1531)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1311)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1120)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1113)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:178)
	at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:72)
	at org.apache.hive.service.cli.operation.SQLOperation$2$1.run(SQLOperation.java:232)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
	at org.apache.hive.service.cli.operation.SQLOperation$2.run(SQLOperation.java:245)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

 INFO HiveServer2-Background-Pool: Thread-297 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=create_database start=1480463797969 end=1480463797985 duration=16 from=org.apache.hadoop.hive.metastore.RetryingHMSHandler threadId=1 retryCount=-1 error=true>
 INFO HiveServer2-Background-Pool: Thread-297 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=runTasks start=1480463797939 end=1480463797985 duration=46 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-297 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=Driver.execute start=1480463797935 end=1480463797985 duration=50 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-297 org.apache.hadoop.hive.ql.Driver - Completed executing command(queryId=gvanvuuren_20161130005656_d9e6b0f2-51a1-48e4-bd79-6cf6a313ff4a); Time taken: 0.05 seconds
OK
 INFO HiveServer2-Background-Pool: Thread-297 org.apache.hadoop.hive.ql.Driver - OK
 INFO HiveServer2-Background-Pool: Thread-297 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-297 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=releaseLocks start=1480463797985 end=1480463797985 duration=0 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-297 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=Driver.run start=1480463797934 end=1480463797985 duration=51 from=org.apache.hadoop.hive.ql.Driver>
 INFO main pseidon-hdfs.hive - Creating hive connection:  jdbc:hive2://localhost:10000 hive 
 INFO main org.apache.hive.jdbc.Utils - Supplied authorities: localhost:10000
 INFO main org.apache.hive.jdbc.Utils - Resolved authority: localhost:10000
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hive.service.cli.thrift.ThriftCLIService - Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V7
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.ql.session.SessionState - Created local directory: /var/folders/tk/w8__4rl57830l94b5y2l7g6w0000gn/T/45b13308-a75a-4603-be4d-1d4f54922c72_resources
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/hive/45b13308-a75a-4603-be4d-1d4f54922c72
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.ql.session.SessionState - Created local directory: /var/folders/tk/w8__4rl57830l94b5y2l7g6w0000gn/T/gvanvuuren/45b13308-a75a-4603-be4d-1d4f54922c72
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/hive/45b13308-a75a-4603-be4d-1d4f54922c72/_tmp_space.db
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.ql.session.SessionState - No Tez session required at this point. hive.execution.engine=mr.
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hive.service.cli.session.HiveSessionImpl - Operation log session directory is created: /var/folders/tk/w8__4rl57830l94b5y2l7g6w0000gn/T/gvanvuuren/operation_logs/45b13308-a75a-4603-be4d-1d4f54922c72
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hive.service.cli.thrift.ThriftCLIService - Opened a session, current sessions: 2
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.ql.Driver - Compiling command(queryId=gvanvuuren_20161130005656_173cb572-2e94-4663-a151-8eec5377de10): CREATE TABLE IF NOT EXISTS pseidon.mytopic_test (tname string) PARTITIONED BY (dt string, hr string)
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=parse start=1480463798057 end=1480463798060 duration=3 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.ql.parse.CalcitePlanner - Starting Semantic Analysis
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.ql.parse.CalcitePlanner - Creating table pseidon.mytopic_test position=27
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=get_table from=org.apache.hadoop.hive.metastore.RetryingHMSHandler>
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.metastore.HiveMetaStore - 2: get_table : db=pseidon tbl=mytopic_test
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=hive	ip=unknown-ip-addr	cmd=get_table : db=pseidon tbl=mytopic_test	
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.metastore.HiveMetaStore - 2: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
 INFO HiveServer2-Handler-Pool: Thread-300 DataNucleus.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is DERBY
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=get_table start=1480463798096 end=1480463798525 duration=429 from=org.apache.hadoop.hive.metastore.RetryingHMSHandler threadId=2 retryCount=0 error=false>
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.ql.Driver - Semantic Analysis Completed
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=semanticAnalyze start=1480463798061 end=1480463798527 duration=466 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.ql.Driver - Returning Hive schema: Schema(fieldSchemas:null, properties:null)
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=compile start=1480463798057 end=1480463798528 duration=471 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=compile start=1480463798057 end=1480463798528 duration=471 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.ql.Driver - Completed compiling command(queryId=gvanvuuren_20161130005656_173cb572-2e94-4663-a151-8eec5377de10); Time taken: 0.471 seconds
 INFO HiveServer2-Background-Pool: Thread-306 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-306 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-306 org.apache.hadoop.hive.ql.Driver - Concurrency mode is disabled, not creating a lock manager
 INFO HiveServer2-Background-Pool: Thread-306 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-306 org.apache.hadoop.hive.ql.Driver - Executing command(queryId=gvanvuuren_20161130005656_173cb572-2e94-4663-a151-8eec5377de10): CREATE TABLE IF NOT EXISTS pseidon.mytopic_test (tname string) PARTITIONED BY (dt string, hr string)
 INFO HiveServer2-Background-Pool: Thread-306 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=TimeToSubmit start=1480463798529 end=1480463798530 duration=1 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-306 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-306 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=runTasks start=1480463798530 end=1480463798530 duration=0 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-306 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=Driver.execute start=1480463798530 end=1480463798530 duration=0 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-306 org.apache.hadoop.hive.ql.Driver - Completed executing command(queryId=gvanvuuren_20161130005656_173cb572-2e94-4663-a151-8eec5377de10); Time taken: 0.0 seconds
OK
 INFO HiveServer2-Background-Pool: Thread-306 org.apache.hadoop.hive.ql.Driver - OK
 INFO HiveServer2-Background-Pool: Thread-306 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-306 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=releaseLocks start=1480463798531 end=1480463798531 duration=0 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-306 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=Driver.run start=1480463798529 end=1480463798531 duration=2 from=org.apache.hadoop.hive.ql.Driver>
 INFO main pseidon-hdfs.hive - Creating hive connection:  jdbc:hive2://localhost:10000 hive 
 INFO main org.apache.hive.jdbc.Utils - Supplied authorities: localhost:10000
 INFO main org.apache.hive.jdbc.Utils - Resolved authority: localhost:10000
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hive.service.cli.thrift.ThriftCLIService - Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V7
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.ql.session.SessionState - Created local directory: /var/folders/tk/w8__4rl57830l94b5y2l7g6w0000gn/T/fad339c6-2c12-4821-9932-ece9dc55ab65_resources
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/hive/fad339c6-2c12-4821-9932-ece9dc55ab65
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.ql.session.SessionState - Created local directory: /var/folders/tk/w8__4rl57830l94b5y2l7g6w0000gn/T/gvanvuuren/fad339c6-2c12-4821-9932-ece9dc55ab65
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/hive/fad339c6-2c12-4821-9932-ece9dc55ab65/_tmp_space.db
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.ql.session.SessionState - No Tez session required at this point. hive.execution.engine=mr.
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hive.service.cli.session.HiveSessionImpl - Operation log session directory is created: /var/folders/tk/w8__4rl57830l94b5y2l7g6w0000gn/T/gvanvuuren/operation_logs/fad339c6-2c12-4821-9932-ece9dc55ab65
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hive.service.cli.thrift.ThriftCLIService - Opened a session, current sessions: 3
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.ql.Driver - Compiling command(queryId=gvanvuuren_20161130005656_4f6e2bad-c9e9-4eec-babc-eb8a04de8960): CREATE TABLE IF NOT EXISTS pseidon.hive_metrics (tname string) PARTITIONED BY (dt string, hr string)
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=parse start=1480463798596 end=1480463798597 duration=1 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.ql.parse.CalcitePlanner - Starting Semantic Analysis
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.ql.parse.CalcitePlanner - Creating table pseidon.hive_metrics position=27
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=get_table from=org.apache.hadoop.hive.metastore.RetryingHMSHandler>
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.metastore.HiveMetaStore - 3: get_table : db=pseidon tbl=hive_metrics
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=hive	ip=unknown-ip-addr	cmd=get_table : db=pseidon tbl=hive_metrics	
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.metastore.HiveMetaStore - 3: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
 INFO HiveServer2-Handler-Pool: Thread-307 DataNucleus.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is DERBY
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=get_table start=1480463798598 end=1480463798615 duration=17 from=org.apache.hadoop.hive.metastore.RetryingHMSHandler threadId=3 retryCount=0 error=false>
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.ql.Driver - Semantic Analysis Completed
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=semanticAnalyze start=1480463798597 end=1480463798615 duration=18 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.ql.Driver - Returning Hive schema: Schema(fieldSchemas:null, properties:null)
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=compile start=1480463798596 end=1480463798615 duration=19 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=compile start=1480463798596 end=1480463798615 duration=19 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.ql.Driver - Completed compiling command(queryId=gvanvuuren_20161130005656_4f6e2bad-c9e9-4eec-babc-eb8a04de8960); Time taken: 0.019 seconds
 INFO HiveServer2-Background-Pool: Thread-313 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-313 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-313 org.apache.hadoop.hive.ql.Driver - Concurrency mode is disabled, not creating a lock manager
 INFO HiveServer2-Background-Pool: Thread-313 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-313 org.apache.hadoop.hive.ql.Driver - Executing command(queryId=gvanvuuren_20161130005656_4f6e2bad-c9e9-4eec-babc-eb8a04de8960): CREATE TABLE IF NOT EXISTS pseidon.hive_metrics (tname string) PARTITIONED BY (dt string, hr string)
 INFO HiveServer2-Background-Pool: Thread-313 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=TimeToSubmit start=1480463798617 end=1480463798617 duration=0 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-313 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-313 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=runTasks start=1480463798617 end=1480463798617 duration=0 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-313 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=Driver.execute start=1480463798617 end=1480463798618 duration=1 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-313 org.apache.hadoop.hive.ql.Driver - Completed executing command(queryId=gvanvuuren_20161130005656_4f6e2bad-c9e9-4eec-babc-eb8a04de8960); Time taken: 0.001 seconds
OK
 INFO HiveServer2-Background-Pool: Thread-313 org.apache.hadoop.hive.ql.Driver - OK
 INFO HiveServer2-Background-Pool: Thread-313 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-313 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=releaseLocks start=1480463798618 end=1480463798618 duration=0 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-313 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=Driver.run start=1480463798617 end=1480463798618 duration=1 from=org.apache.hadoop.hive.ql.Driver>
 INFO main pseidon-hdfs.hive - Creating hive connection:  jdbc:hive2://localhost:10000 hive 
 INFO main org.apache.hive.jdbc.Utils - Supplied authorities: localhost:10000
 INFO main org.apache.hive.jdbc.Utils - Resolved authority: localhost:10000
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hive.service.cli.thrift.ThriftCLIService - Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V7
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.ql.session.SessionState - Created local directory: /var/folders/tk/w8__4rl57830l94b5y2l7g6w0000gn/T/55608edd-0696-4f94-aea2-8dae64df6d49_resources
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/hive/55608edd-0696-4f94-aea2-8dae64df6d49
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.ql.session.SessionState - Created local directory: /var/folders/tk/w8__4rl57830l94b5y2l7g6w0000gn/T/gvanvuuren/55608edd-0696-4f94-aea2-8dae64df6d49
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/hive/55608edd-0696-4f94-aea2-8dae64df6d49/_tmp_space.db
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.ql.session.SessionState - No Tez session required at this point. hive.execution.engine=mr.
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hive.service.cli.session.HiveSessionImpl - Operation log session directory is created: /var/folders/tk/w8__4rl57830l94b5y2l7g6w0000gn/T/gvanvuuren/operation_logs/55608edd-0696-4f94-aea2-8dae64df6d49
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hive.service.cli.thrift.ThriftCLIService - Opened a session, current sessions: 4
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.ql.Driver - Compiling command(queryId=gvanvuuren_20161130005656_27780821-5d6c-4f9e-9999-e7a735e8f5c3): CREATE TABLE IF NOT EXISTS pseidon.hdfs_metrics (tname string) PARTITIONED BY (dt string, hr string)
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=parse start=1480463798677 end=1480463798677 duration=0 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.ql.parse.CalcitePlanner - Starting Semantic Analysis
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.ql.parse.CalcitePlanner - Creating table pseidon.hdfs_metrics position=27
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=get_table from=org.apache.hadoop.hive.metastore.RetryingHMSHandler>
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.metastore.HiveMetaStore - 4: get_table : db=pseidon tbl=hdfs_metrics
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=hive	ip=unknown-ip-addr	cmd=get_table : db=pseidon tbl=hdfs_metrics	
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.metastore.HiveMetaStore - 4: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
 INFO HiveServer2-Handler-Pool: Thread-314 DataNucleus.Query - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is DERBY
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=get_table start=1480463798677 end=1480463798698 duration=21 from=org.apache.hadoop.hive.metastore.RetryingHMSHandler threadId=4 retryCount=0 error=false>
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.ql.Driver - Semantic Analysis Completed
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=semanticAnalyze start=1480463798677 end=1480463798698 duration=21 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.ql.Driver - Returning Hive schema: Schema(fieldSchemas:null, properties:null)
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=compile start=1480463798676 end=1480463798698 duration=22 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=compile start=1480463798676 end=1480463798698 duration=22 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.ql.Driver - Completed compiling command(queryId=gvanvuuren_20161130005656_27780821-5d6c-4f9e-9999-e7a735e8f5c3); Time taken: 0.022 seconds
 INFO HiveServer2-Background-Pool: Thread-320 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-320 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-320 org.apache.hadoop.hive.ql.Driver - Concurrency mode is disabled, not creating a lock manager
 INFO HiveServer2-Background-Pool: Thread-320 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-320 org.apache.hadoop.hive.ql.Driver - Executing command(queryId=gvanvuuren_20161130005656_27780821-5d6c-4f9e-9999-e7a735e8f5c3): CREATE TABLE IF NOT EXISTS pseidon.hdfs_metrics (tname string) PARTITIONED BY (dt string, hr string)
 INFO HiveServer2-Background-Pool: Thread-320 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=TimeToSubmit start=1480463798700 end=1480463798701 duration=1 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-320 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-320 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=runTasks start=1480463798701 end=1480463798701 duration=0 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-320 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=Driver.execute start=1480463798700 end=1480463798701 duration=1 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-320 org.apache.hadoop.hive.ql.Driver - Completed executing command(queryId=gvanvuuren_20161130005656_27780821-5d6c-4f9e-9999-e7a735e8f5c3); Time taken: 0.001 seconds
OK
 INFO HiveServer2-Background-Pool: Thread-320 org.apache.hadoop.hive.ql.Driver - OK
 INFO HiveServer2-Background-Pool: Thread-320 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-320 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=releaseLocks start=1480463798701 end=1480463798701 duration=0 from=org.apache.hadoop.hive.ql.Driver>
 INFO HiveServer2-Background-Pool: Thread-320 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=Driver.run start=1480463798700 end=1480463798701 duration=1 from=org.apache.hadoop.hive.ql.Driver>
Formatting using clusterid: testClusterID
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - No KeyProvider found.
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - fsLock is fair:true
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager - dfs.block.invalidate.limit=1000
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager - dfs.namenode.datanode.registration.ip-hostname-check=true
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - The block deletion will start around 2016 Nov 30 00:56:38
 INFO main org.apache.hadoop.util.GSet - Computing capacity for map BlocksMap
 INFO main org.apache.hadoop.util.GSet - VM type       = 64-bit
 INFO main org.apache.hadoop.util.GSet - 2.0% max memory 3.6 GB = 72.8 MB
 INFO main org.apache.hadoop.util.GSet - capacity      = 2^23 = 8388608 entries
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - dfs.block.access.token.enable=false
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - defaultReplication         = 1
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - maxReplication             = 512
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - minReplication             = 1
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - maxReplicationStreams      = 2
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - replicationRecheckInterval = 3000
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - encryptDataTransfer        = false
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - maxNumBlocksToLog          = 1000
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - fsOwner             = gvanvuuren (auth:SIMPLE)
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - supergroup          = supergroup
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - isPermissionEnabled = true
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - HA Enabled: false
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Append Enabled: true
 INFO main org.apache.hadoop.util.GSet - Computing capacity for map INodeMap
 INFO main org.apache.hadoop.util.GSet - VM type       = 64-bit
 INFO main org.apache.hadoop.util.GSet - 1.0% max memory 3.6 GB = 36.4 MB
 INFO main org.apache.hadoop.util.GSet - capacity      = 2^22 = 4194304 entries
 INFO main org.apache.hadoop.hdfs.server.namenode.NameNode - Caching file names occuring more than 10 times
 INFO main org.apache.hadoop.util.GSet - Computing capacity for map cachedBlocks
 INFO main org.apache.hadoop.util.GSet - VM type       = 64-bit
 INFO main org.apache.hadoop.util.GSet - 0.25% max memory 3.6 GB = 9.1 MB
 INFO main org.apache.hadoop.util.GSet - capacity      = 2^20 = 1048576 entries
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - dfs.namenode.safemode.min.datanodes = 0
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - dfs.namenode.safemode.extension     = 0
 INFO main org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics - NNTop conf: dfs.namenode.top.window.num.buckets = 10
 INFO main org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics - NNTop conf: dfs.namenode.top.num.users = 10
 INFO main org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Retry cache on namenode is enabled
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
 INFO main org.apache.hadoop.util.GSet - Computing capacity for map NameNodeRetryCache
 INFO main org.apache.hadoop.util.GSet - VM type       = 64-bit
 INFO main org.apache.hadoop.util.GSet - 0.029999999329447746% max memory 3.6 GB = 1.1 MB
 INFO main org.apache.hadoop.util.GSet - capacity      = 2^17 = 131072 entries
 INFO main org.apache.hadoop.hdfs.server.namenode.NNConf - ACLs enabled? false
 INFO main org.apache.hadoop.hdfs.server.namenode.NNConf - XAttrs enabled? true
 INFO main org.apache.hadoop.hdfs.server.namenode.NNConf - Maximum size of an xattr: 16384
 INFO main org.apache.hadoop.hdfs.server.namenode.FSImage - Allocated new BlockPoolId: BP-1586230954-10.0.0.10-1480463798732
 INFO main org.apache.hadoop.hdfs.server.common.Storage - Storage directory /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/name1 has been successfully formatted.
 INFO main org.apache.hadoop.hdfs.server.common.Storage - Storage directory /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/name2 has been successfully formatted.
 INFO main org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager - Going to retain 1 images with txid >= 0
 INFO main org.apache.hadoop.hdfs.server.namenode.NameNode - createNameNode []
 INFO main org.apache.hadoop.metrics2.impl.MetricsSystemImpl - NameNode metrics system started (again)
 INFO main org.apache.hadoop.hdfs.server.namenode.NameNode - fs.defaultFS is hdfs://127.0.0.1:0
 INFO org.apache.hadoop.util.JvmPauseMonitor$Monitor@ce24a98 org.apache.hadoop.util.JvmPauseMonitor - Starting JVM pause monitor
 INFO main org.apache.hadoop.hdfs.DFSUtil - Starting Web-server for hdfs at: http://localhost:0
 INFO main org.apache.hadoop.security.authentication.server.AuthenticationFilter - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
 INFO main org.apache.hadoop.http.HttpRequestLog - Http request log for http.requests.namenode is not defined
 INFO main org.apache.hadoop.http.HttpServer2 - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
 INFO main org.apache.hadoop.http.HttpServer2 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
 INFO main org.apache.hadoop.http.HttpServer2 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
 INFO main org.apache.hadoop.http.HttpServer2 - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
 INFO main org.apache.hadoop.http.HttpServer2 - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
 INFO main org.apache.hadoop.http.HttpServer2 - Jetty bound to port 55015
 INFO main org.mortbay.log - jetty-6.1.26.cloudera.4
 INFO main org.mortbay.log - Extract jar:file:/Users/gvanvuuren/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar!/webapps/hdfs to /var/folders/tk/w8__4rl57830l94b5y2l7g6w0000gn/T/Jetty_localhost_55015_hdfs____.vawrs2/webapp
 INFO main org.mortbay.log - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:55015
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - No KeyProvider found.
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - fsLock is fair:true
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager - dfs.block.invalidate.limit=1000
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager - dfs.namenode.datanode.registration.ip-hostname-check=true
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - The block deletion will start around 2016 Nov 30 00:56:38
 INFO main org.apache.hadoop.util.GSet - Computing capacity for map BlocksMap
 INFO main org.apache.hadoop.util.GSet - VM type       = 64-bit
 INFO main org.apache.hadoop.util.GSet - 2.0% max memory 3.6 GB = 72.8 MB
 INFO main org.apache.hadoop.util.GSet - capacity      = 2^23 = 8388608 entries
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - dfs.block.access.token.enable=false
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - defaultReplication         = 1
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - maxReplication             = 512
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - minReplication             = 1
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - maxReplicationStreams      = 2
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - replicationRecheckInterval = 3000
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - encryptDataTransfer        = false
 INFO main org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - maxNumBlocksToLog          = 1000
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - fsOwner             = gvanvuuren (auth:SIMPLE)
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - supergroup          = supergroup
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - isPermissionEnabled = true
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - HA Enabled: false
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Append Enabled: true
 INFO main org.apache.hadoop.util.GSet - Computing capacity for map INodeMap
 INFO main org.apache.hadoop.util.GSet - VM type       = 64-bit
 INFO main org.apache.hadoop.util.GSet - 1.0% max memory 3.6 GB = 36.4 MB
 INFO main org.apache.hadoop.util.GSet - capacity      = 2^22 = 4194304 entries
 INFO main org.apache.hadoop.hdfs.server.namenode.NameNode - Caching file names occuring more than 10 times
 INFO main org.apache.hadoop.util.GSet - Computing capacity for map cachedBlocks
 INFO main org.apache.hadoop.util.GSet - VM type       = 64-bit
 INFO main org.apache.hadoop.util.GSet - 0.25% max memory 3.6 GB = 9.1 MB
 INFO main org.apache.hadoop.util.GSet - capacity      = 2^20 = 1048576 entries
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - dfs.namenode.safemode.min.datanodes = 0
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - dfs.namenode.safemode.extension     = 0
 INFO main org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics - NNTop conf: dfs.namenode.top.window.num.buckets = 10
 INFO main org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics - NNTop conf: dfs.namenode.top.num.users = 10
 INFO main org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Retry cache on namenode is enabled
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
 INFO main org.apache.hadoop.util.GSet - Computing capacity for map NameNodeRetryCache
 INFO main org.apache.hadoop.util.GSet - VM type       = 64-bit
 INFO main org.apache.hadoop.util.GSet - 0.029999999329447746% max memory 3.6 GB = 1.1 MB
 INFO main org.apache.hadoop.util.GSet - capacity      = 2^17 = 131072 entries
 INFO main org.apache.hadoop.hdfs.server.namenode.NNConf - ACLs enabled? false
 INFO main org.apache.hadoop.hdfs.server.namenode.NNConf - XAttrs enabled? true
 INFO main org.apache.hadoop.hdfs.server.namenode.NNConf - Maximum size of an xattr: 16384
 INFO main org.apache.hadoop.hdfs.server.common.Storage - Lock on /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/name1/in_use.lock acquired by nodename 9517@Gerrrits-MacBook-Pro.local
 INFO main org.apache.hadoop.hdfs.server.common.Storage - Lock on /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/name2/in_use.lock acquired by nodename 9517@Gerrrits-MacBook-Pro.local
 INFO main org.apache.hadoop.hdfs.server.namenode.FileJournalManager - Recovering unfinalized segments in /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/name1/current
 INFO main org.apache.hadoop.hdfs.server.namenode.FileJournalManager - Recovering unfinalized segments in /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/name2/current
 INFO main org.apache.hadoop.hdfs.server.namenode.FSImage - No edit log streams selected.
 INFO main org.apache.hadoop.hdfs.server.namenode.FSImage - Planning to load image: FSImageFile(file=/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/name1/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
 INFO main org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode - Loading 1 INodes.
 INFO main org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf - Loaded FSImage in 0 seconds.
 INFO main org.apache.hadoop.hdfs.server.namenode.FSImage - Loaded image for txid 0 from /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/name1/current/fsimage_0000000000000000000
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
 INFO main org.apache.hadoop.hdfs.server.namenode.FSEditLog - Starting log segment at 1
 INFO main org.apache.hadoop.hdfs.server.namenode.NameCache - initialized with 0 entries 0 lookups
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Finished loading FSImage in 10 msecs
 INFO main org.apache.hadoop.hdfs.server.namenode.NameNode - RPC server is binding to localhost:0
 INFO main org.apache.hadoop.ipc.CallQueueManager - Using callQueue class java.util.concurrent.LinkedBlockingQueue
 INFO Socket Reader #1 for port 55016 org.apache.hadoop.ipc.Server - Starting Socket Reader #1 for port 55016
 INFO main org.apache.hadoop.hdfs.server.namenode.NameNode - Clients are to use localhost:55016 to access this namenode/service.
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Registered FSNamesystemState MBean
 INFO main org.apache.hadoop.hdfs.server.namenode.LeaseManager - Number of blocks under construction: 0
 INFO main org.apache.hadoop.hdfs.server.namenode.LeaseManager - Number of blocks under construction: 0
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - initializing replication queues
 INFO main org.apache.hadoop.hdfs.StateChange - STATE* Leaving safe mode after 0 secs
 INFO main org.apache.hadoop.hdfs.StateChange - STATE* Network topology has 0 racks and 0 datanodes
 INFO main org.apache.hadoop.hdfs.StateChange - STATE* UnderReplicatedBlocks has 0 blocks
 INFO Replication Queue Initializer org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - Total number of blocks            = 0
 INFO Replication Queue Initializer org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - Number of invalid blocks          = 0
 INFO Replication Queue Initializer org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - Number of under-replicated blocks = 0
 INFO Replication Queue Initializer org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - Number of  over-replicated blocks = 0
 INFO Replication Queue Initializer org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - Number of blocks being written    = 0
 INFO Replication Queue Initializer org.apache.hadoop.hdfs.StateChange - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 5 msec
 INFO IPC Server Responder org.apache.hadoop.ipc.Server - IPC Server Responder: starting
 INFO IPC Server listener on 55016 org.apache.hadoop.ipc.Server - IPC Server listener on 55016: starting
 INFO main org.apache.hadoop.hdfs.server.namenode.NameNode - NameNode RPC up at: localhost/127.0.0.1:55016
 INFO main org.apache.hadoop.hdfs.server.namenode.FSNamesystem - Starting services required for active state
 INFO CacheReplicationMonitor(1550324915) org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor - Starting CacheReplicationMonitor with interval 30000 milliseconds
 INFO CacheReplicationMonitor(1550324915) org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor - Rescanning after 137609577 milliseconds
 INFO CacheReplicationMonitor(1550324915) org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor - Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
 INFO main org.apache.hadoop.hdfs.MiniDFSCluster - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1,[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2
 INFO main org.apache.hadoop.metrics2.impl.MetricsSystemImpl - DataNode metrics system started (again)
 INFO main org.apache.hadoop.hdfs.server.datanode.BlockScanner - Initialized block scanner with targetBytesPerSec 1048576
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - Configured hostname is 127.0.0.1
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - Starting DataNode with maxLockedMemory = 0
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - Opened streaming server at /127.0.0.1:55017
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - Balancing bandwith is 1048576 bytes/s
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - Number threads for balancing is 5
 INFO main org.apache.hadoop.security.authentication.server.AuthenticationFilter - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
 INFO main org.apache.hadoop.http.HttpRequestLog - Http request log for http.requests.datanode is not defined
 INFO main org.apache.hadoop.http.HttpServer2 - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
 INFO main org.apache.hadoop.http.HttpServer2 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
 INFO main org.apache.hadoop.http.HttpServer2 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
 INFO main org.apache.hadoop.http.HttpServer2 - Jetty bound to port 55018
 INFO main org.mortbay.log - jetty-6.1.26.cloudera.4
 INFO main org.mortbay.log - Extract jar:file:/Users/gvanvuuren/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.0/hadoop-hdfs-2.6.0-cdh5.7.0-tests.jar!/webapps/datanode to /var/folders/tk/w8__4rl57830l94b5y2l7g6w0000gn/T/Jetty_localhost_55018_datanode____h5li5a/webapp
 INFO main org.mortbay.log - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:55018
 INFO main org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer - Listening HTTP traffic on /127.0.0.1:55019
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - dnUserName = gvanvuuren
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - supergroup = supergroup
 INFO org.apache.hadoop.util.JvmPauseMonitor$Monitor@fed510f org.apache.hadoop.util.JvmPauseMonitor - Starting JVM pause monitor
 INFO main org.apache.hadoop.ipc.CallQueueManager - Using callQueue class java.util.concurrent.LinkedBlockingQueue
 INFO Socket Reader #1 for port 55020 org.apache.hadoop.ipc.Server - Starting Socket Reader #1 for port 55020
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - Opened IPC server at /127.0.0.1:55020
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - Refresh request received for nameservices: null
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - Starting BPOfferServices for nameservices: <default>
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.datanode.DataNode - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:55016 starting to offer service
 INFO IPC Server Responder org.apache.hadoop.ipc.Server - IPC Server Responder: starting
 INFO IPC Server listener on 55020 org.apache.hadoop.ipc.Server - IPC Server listener on 55020: starting
 INFO main org.apache.hadoop.hdfs.MiniDFSCluster - dnInfo.length != numDataNodes
 INFO main org.apache.hadoop.hdfs.MiniDFSCluster - Waiting for cluster to become active
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.common.Storage - Lock on /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/in_use.lock acquired by nodename 9517@Gerrrits-MacBook-Pro.local
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.common.Storage - Storage directory /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1 is not formatted for BP-1586230954-10.0.0.10-1480463798732
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.common.Storage - Formatting ...
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.common.Storage - Analyzing storage directories for bpid BP-1586230954-10.0.0.10-1480463798732
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.common.Storage - Locking is disabled for /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/current/BP-1586230954-10.0.0.10-1480463798732
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.common.Storage - Block pool storage directory /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/current/BP-1586230954-10.0.0.10-1480463798732 is not formatted for BP-1586230954-10.0.0.10-1480463798732
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.common.Storage - Formatting ...
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.common.Storage - Formatting block pool BP-1586230954-10.0.0.10-1480463798732 directory /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/current/BP-1586230954-10.0.0.10-1480463798732/current
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.common.Storage - Lock on /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/in_use.lock acquired by nodename 9517@Gerrrits-MacBook-Pro.local
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.common.Storage - Storage directory /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2 is not formatted for BP-1586230954-10.0.0.10-1480463798732
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.common.Storage - Formatting ...
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.common.Storage - Analyzing storage directories for bpid BP-1586230954-10.0.0.10-1480463798732
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.common.Storage - Locking is disabled for /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/current/BP-1586230954-10.0.0.10-1480463798732
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.common.Storage - Block pool storage directory /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/current/BP-1586230954-10.0.0.10-1480463798732 is not formatted for BP-1586230954-10.0.0.10-1480463798732
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.common.Storage - Formatting ...
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.common.Storage - Formatting block pool BP-1586230954-10.0.0.10-1480463798732 directory /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/current/BP-1586230954-10.0.0.10-1480463798732/current
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.datanode.DataNode - Setting up storage: nsid=748544152;bpid=BP-1586230954-10.0.0.10-1480463798732;lv=-56;nsInfo=lv=-60;cid=testClusterID;nsid=748544152;c=0;bpid=BP-1586230954-10.0.0.10-1480463798732;dnuuid=null
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.datanode.DataNode - Generated and persisted new Datanode UUID fba48ee8-765d-49bb-a1c4-e8a8d4921872
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Added new volume: DS-b21ee017-2b19-4bc6-ad3e-da604daae8ee
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Added volume - /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/current, StorageType: DISK
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Added new volume: DS-a16ab906-4ea8-4205-9d03-d8565272a70a
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Added volume - /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/current, StorageType: DISK
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Registered FSDatasetState MBean
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Volume reference is released.
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Adding block pool BP-1586230954-10.0.0.10-1480463798732
 INFO Thread-278 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Scanning block pool BP-1586230954-10.0.0.10-1480463798732 on volume /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/current...
 INFO Thread-279 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Scanning block pool BP-1586230954-10.0.0.10-1480463798732 on volume /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/current...
 INFO Thread-279 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Time taken to scan block pool BP-1586230954-10.0.0.10-1480463798732 on /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/current: 21ms
 INFO Thread-278 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Time taken to scan block pool BP-1586230954-10.0.0.10-1480463798732 on /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/current: 21ms
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Total time to scan all replicas for block pool BP-1586230954-10.0.0.10-1480463798732: 21ms
 INFO Thread-282 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Adding replicas to map for block pool BP-1586230954-10.0.0.10-1480463798732 on volume /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/current...
 INFO Thread-283 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Adding replicas to map for block pool BP-1586230954-10.0.0.10-1480463798732 on volume /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/current...
 INFO Thread-282 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Time to add replicas to map for block pool BP-1586230954-10.0.0.10-1480463798732 on volume /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/current: 0ms
 INFO Thread-283 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Time to add replicas to map for block pool BP-1586230954-10.0.0.10-1480463798732 on volume /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/current: 0ms
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Total time to add all replicas to map: 1ms
 INFO VolumeScannerThread(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2) org.apache.hadoop.hdfs.server.datanode.VolumeScanner - Now scanning bpid BP-1586230954-10.0.0.10-1480463798732 on volume /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2
 INFO VolumeScannerThread(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1) org.apache.hadoop.hdfs.server.datanode.VolumeScanner - Now scanning bpid BP-1586230954-10.0.0.10-1480463798732 on volume /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.datanode.DirectoryScanner - Periodic Directory Tree Verification scan starting at 1480469347173ms with interval of 21600000ms
 INFO VolumeScannerThread(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2) org.apache.hadoop.hdfs.server.datanode.VolumeScanner - VolumeScanner(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2, DS-a16ab906-4ea8-4205-9d03-d8565272a70a): finished scanning block pool BP-1586230954-10.0.0.10-1480463798732
 INFO VolumeScannerThread(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1) org.apache.hadoop.hdfs.server.datanode.VolumeScanner - VolumeScanner(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1, DS-b21ee017-2b19-4bc6-ad3e-da604daae8ee): finished scanning block pool BP-1586230954-10.0.0.10-1480463798732
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.datanode.DataNode - Block pool BP-1586230954-10.0.0.10-1480463798732 (Datanode Uuid null) service to localhost/127.0.0.1:55016 beginning handshake with NN
 INFO VolumeScannerThread(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2) org.apache.hadoop.hdfs.server.datanode.VolumeScanner - VolumeScanner(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2, DS-a16ab906-4ea8-4205-9d03-d8565272a70a): no suitable block pools found to scan.  Waiting 1814399998 ms.
 INFO VolumeScannerThread(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1) org.apache.hadoop.hdfs.server.datanode.VolumeScanner - VolumeScanner(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1, DS-b21ee017-2b19-4bc6-ad3e-da604daae8ee): no suitable block pools found to scan.  Waiting 1814399998 ms.
 INFO IPC Server handler 2 on 55016 org.apache.hadoop.hdfs.StateChange - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=fba48ee8-765d-49bb-a1c4-e8a8d4921872, infoPort=55019, infoSecurePort=0, ipcPort=55020, storageInfo=lv=-56;cid=testClusterID;nsid=748544152;c=0) storage fba48ee8-765d-49bb-a1c4-e8a8d4921872
 INFO IPC Server handler 2 on 55016 org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor - Number of failed storage changes from 0 to 0
 INFO IPC Server handler 2 on 55016 org.apache.hadoop.net.NetworkTopology - Adding a new node: /default-rack/127.0.0.1:55017
 INFO IPC Server handler 2 on 55016 org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager - Registered DN fba48ee8-765d-49bb-a1c4-e8a8d4921872 (127.0.0.1:55017).
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.datanode.DataNode - Block pool Block pool BP-1586230954-10.0.0.10-1480463798732 (Datanode Uuid null) service to localhost/127.0.0.1:55016 successfully registered with NN
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.datanode.DataNode - For namenode localhost/127.0.0.1:55016 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
 INFO IPC Server handler 3 on 55016 org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor - Number of failed storage changes from 0 to 0
 INFO IPC Server handler 3 on 55016 org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor - Adding new storage ID DS-b21ee017-2b19-4bc6-ad3e-da604daae8ee for DN 127.0.0.1:55017
 INFO IPC Server handler 3 on 55016 org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor - Adding new storage ID DS-a16ab906-4ea8-4205-9d03-d8565272a70a for DN 127.0.0.1:55017
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.datanode.DataNode - Namenode Block pool BP-1586230954-10.0.0.10-1480463798732 (Datanode Uuid fba48ee8-765d-49bb-a1c4-e8a8d4921872) service to localhost/127.0.0.1:55016 trying to claim ACTIVE state with txid=1
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.datanode.DataNode - Acknowledging ACTIVE Namenode Block pool BP-1586230954-10.0.0.10-1480463798732 (Datanode Uuid fba48ee8-765d-49bb-a1c4-e8a8d4921872) service to localhost/127.0.0.1:55016
 INFO IPC Server handler 4 on 55016 org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - Processing first storage report for DS-a16ab906-4ea8-4205-9d03-d8565272a70a from datanode fba48ee8-765d-49bb-a1c4-e8a8d4921872
 INFO IPC Server handler 4 on 55016 BlockStateChange - BLOCK* processReport: from storage DS-a16ab906-4ea8-4205-9d03-d8565272a70a node DatanodeRegistration(127.0.0.1, datanodeUuid=fba48ee8-765d-49bb-a1c4-e8a8d4921872, infoPort=55019, infoSecurePort=0, ipcPort=55020, storageInfo=lv=-56;cid=testClusterID;nsid=748544152;c=0), blocks: 0, hasStaleStorage: true, processing time: 0 msecs
 INFO IPC Server handler 4 on 55016 org.apache.hadoop.hdfs.server.blockmanagement.BlockManager - Processing first storage report for DS-b21ee017-2b19-4bc6-ad3e-da604daae8ee from datanode fba48ee8-765d-49bb-a1c4-e8a8d4921872
 INFO IPC Server handler 4 on 55016 BlockStateChange - BLOCK* processReport: from storage DS-b21ee017-2b19-4bc6-ad3e-da604daae8ee node DatanodeRegistration(127.0.0.1, datanodeUuid=fba48ee8-765d-49bb-a1c4-e8a8d4921872, infoPort=55019, infoSecurePort=0, ipcPort=55020, storageInfo=lv=-56;cid=testClusterID;nsid=748544152;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.datanode.DataNode - Successfully sent block report 0x44c59b33ad620c87,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.datanode.DataNode - Got finalize command for block pool BP-1586230954-10.0.0.10-1480463798732
 INFO main org.apache.hadoop.hdfs.MiniDFSCluster - Cluster is active
 INFO main org.apache.hadoop.hdfs.MiniDFSCluster - Cluster is active
 INFO main com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Started.
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hive.service.cli.thrift.ThriftCLIService - Session disconnected without closing properly, close it now
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hive.service.cli.thrift.ThriftCLIService - Session disconnected without closing properly, close it now
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hive.service.cli.thrift.ThriftCLIService - Session disconnected without closing properly, close it now
 INFO HiveServer2-Handler-Pool: Thread-291 org.apache.hive.service.cli.thrift.ThriftCLIService - Session disconnected without closing properly, close it now
 INFO HiveServer2-Handler-Pool: Thread-291 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=shutdown from=org.apache.hadoop.hive.metastore.RetryingHMSHandler>
 INFO HiveServer2-Handler-Pool: Thread-291 org.apache.hadoop.hive.metastore.HiveMetaStore - 5: Shutting down the object store...
 INFO HiveServer2-Handler-Pool: Thread-291 org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=hive	ip=unknown-ip-addr	cmd=Shutting down the object store...	
 INFO HiveServer2-Handler-Pool: Thread-291 org.apache.hadoop.hive.metastore.HiveMetaStore - 5: Metastore shutdown complete.
 INFO HiveServer2-Handler-Pool: Thread-291 org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=hive	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
 INFO HiveServer2-Handler-Pool: Thread-291 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=shutdown start=1480463799330 end=1480463799331 duration=1 from=org.apache.hadoop.hive.metastore.RetryingHMSHandler threadId=5 retryCount=0 error=false>
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=shutdown from=org.apache.hadoop.hive.metastore.RetryingHMSHandler>
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.metastore.HiveMetaStore - 3: Shutting down the object store...
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=hive	ip=unknown-ip-addr	cmd=Shutting down the object store...	
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.metastore.HiveMetaStore - 3: Metastore shutdown complete.
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=hive	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
 INFO HiveServer2-Handler-Pool: Thread-307 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=shutdown start=1480463799331 end=1480463799332 duration=1 from=org.apache.hadoop.hive.metastore.RetryingHMSHandler threadId=3 retryCount=0 error=false>
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=shutdown from=org.apache.hadoop.hive.metastore.RetryingHMSHandler>
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.metastore.HiveMetaStore - 2: Shutting down the object store...
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=hive	ip=unknown-ip-addr	cmd=Shutting down the object store...	
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.metastore.HiveMetaStore - 2: Metastore shutdown complete.
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=hive	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
 INFO HiveServer2-Handler-Pool: Thread-300 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=shutdown start=1480463799332 end=1480463799333 duration=1 from=org.apache.hadoop.hive.metastore.RetryingHMSHandler threadId=2 retryCount=0 error=false>
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.ql.log.PerfLogger - <PERFLOG method=shutdown from=org.apache.hadoop.hive.metastore.RetryingHMSHandler>
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.metastore.HiveMetaStore - 4: Shutting down the object store...
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=hive	ip=unknown-ip-addr	cmd=Shutting down the object store...	
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.metastore.HiveMetaStore - 4: Metastore shutdown complete.
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=hive	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
 INFO HiveServer2-Handler-Pool: Thread-314 org.apache.hadoop.hive.ql.log.PerfLogger - </PERFLOG method=shutdown start=1480463799333 end=1480463799333 duration=0 from=org.apache.hadoop.hive.metastore.RetryingHMSHandler threadId=4 retryCount=0 error=false>
 INFO main hsqldb.db.HSQLDB58B284D522.ENGINE - Checkpoint start
 INFO main hsqldb.db.HSQLDB58B284D522.ENGINE - checkpointClose start
 INFO main hsqldb.db.HSQLDB58B284D522.ENGINE - checkpointClose end
 INFO main hsqldb.db.HSQLDB58B284D522.ENGINE - Checkpoint end - txts: 1
 INFO main com.zaxxer.hikari.pool.PoolBase - HikariPool-1 - Driver does not support get/set network timeout for connections. (feature not supported)
 INFO main com.zaxxer.hikari.HikariDataSource - HikariPool-2 - Started.
 INFO main com.zaxxer.hikari.pool.PoolBase - HikariPool-2 - Driver does not support get/set network timeout for connections. (feature not supported)
 INFO main fileape.core - start file check  2000  rollover-sise  115000000  rollover-timeout  600000
 INFO hdfs-copy-0 pseidon-hdfs.hdfs-copy-service - Copy files  1
 INFO hdfs-copy-0 pseidon-hdfs.hdfs-copy-service - Copy thread-exec:  #object[java.util.concurrent.ScheduledThreadPoolExecutor 0x7b6f280 java.util.concurrent.ScheduledThreadPoolExecutor@7b6f280[Running, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 0]]
 INFO hdfs-copy-1 org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
 INFO hdfs-copy-1 pseidon-hdfs.hdfs-copy-service - load-topic-meta!: query  select base_partition,
                        log_partition,
                        hive_url,
                        hive_table_name,
                        quarantine,
                        hive_user,
                        hive_password
                        from pseidon_logs where log='mytopic-test'
 INFO hdfs-copy-1 pseidon-hdfs.hdfs-copy-service - copying file  #object[java.io.File 0x5ec3e3b3 target/hdfs-copy-service-int-test/1480463791632/mytopic-test-2016-05-17-00.gz]  via  /tmp/copying_log_pseidon_mytopic-test_dt=20160517_hr=2016051700_Gerrrits-MacBook-Pro.local-mytopic-test-2016-05-17-00.gz  to  /log/pseidon/mytopic-test/dt=20160517/hr=2016051700/Gerrrits-MacBook-Pro.local-mytopic-test-2016-05-17-00.gz
 INFO IPC Server handler 9 on 55016 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/log/pseidon/mytopic-test/dt=20160517/hr=2016051700	dst=null	perm=null	proto=rpc
 INFO hdfs-copy-1 pseidon-hdfs.hdfs-util - creating hdfs dir  /log/pseidon/mytopic-test/dt=20160517/hr=2016051700
 INFO IPC Server handler 7 on 55016 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/log/pseidon/mytopic-test/dt=20160517/hr=2016051700	dst=null	perm=gvanvuuren:supergroup:rwxr-xr-x	proto=rpc
 INFO hdfs-copy-1 pseidon-hdfs.hdfs-util - calling on-create
 INFO IPC Server handler 8 on 55016 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tmp/copying_log_pseidon_mytopic-test_dt=20160517_hr=2016051700_Gerrrits-MacBook-Pro.local-mytopic-test-2016-05-17-00.gz	dst=null	perm=null	proto=rpc
 INFO IPC Server handler 0 on 55016 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/tmp/copying_log_pseidon_mytopic-test_dt=20160517_hr=2016051700_Gerrrits-MacBook-Pro.local-mytopic-test-2016-05-17-00.gz	dst=null	perm=gvanvuuren:supergroup:rw-r--r--	proto=rpc
 INFO IPC Server handler 1 on 55016 org.apache.hadoop.hdfs.StateChange - BLOCK* allocateBlock: /tmp/copying_log_pseidon_mytopic-test_dt=20160517_hr=2016051700_Gerrrits-MacBook-Pro.local-mytopic-test-2016-05-17-00.gz. BP-1586230954-10.0.0.10-1480463798732 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-a16ab906-4ea8-4205-9d03-d8565272a70a:NORMAL:127.0.0.1:55017|RBW]]}
 INFO DataXceiver for client DFSClient_NONMAPREDUCE_-636513846_1 at /127.0.0.1:55023 [Receiving block BP-1586230954-10.0.0.10-1480463798732:blk_1073741825_1001] org.apache.hadoop.hdfs.server.datanode.DataNode - Receiving BP-1586230954-10.0.0.10-1480463798732:blk_1073741825_1001 src: /127.0.0.1:55023 dest: /127.0.0.1:55017
 INFO PacketResponder: BP-1586230954-10.0.0.10-1480463798732:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace - src: /127.0.0.1:55023, dest: /127.0.0.1:55017, bytes: 19, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-636513846_1, offset: 0, srvID: fba48ee8-765d-49bb-a1c4-e8a8d4921872, blockid: BP-1586230954-10.0.0.10-1480463798732:blk_1073741825_1001, duration: 1732665
 INFO PacketResponder: BP-1586230954-10.0.0.10-1480463798732:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] org.apache.hadoop.hdfs.server.datanode.DataNode - PacketResponder: BP-1586230954-10.0.0.10-1480463798732:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
 INFO IPC Server handler 3 on 55016 BlockStateChange - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:55017 is added to blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-b21ee017-2b19-4bc6-ad3e-da604daae8ee:NORMAL:127.0.0.1:55017|FINALIZED]]} size 0
 INFO IPC Server handler 4 on 55016 org.apache.hadoop.hdfs.StateChange - DIR* completeFile: /tmp/copying_log_pseidon_mytopic-test_dt=20160517_hr=2016051700_Gerrrits-MacBook-Pro.local-mytopic-test-2016-05-17-00.gz is closed by DFSClient_NONMAPREDUCE_-636513846_1
 INFO IPC Server handler 5 on 55016 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=rename	src=/tmp/copying_log_pseidon_mytopic-test_dt=20160517_hr=2016051700_Gerrrits-MacBook-Pro.local-mytopic-test-2016-05-17-00.gz	dst=/log/pseidon/mytopic-test/dt=20160517/hr=2016051700/Gerrrits-MacBook-Pro.local-mytopic-test-2016-05-17-00.gz	perm=gvanvuuren:supergroup:rw-r--r--	proto=rpc
 INFO hdfs-copy-1 pseidon-hdfs.hdfs-copy-service - Deleting file  #object[java.io.File 0x5ec3e3b3 target/hdfs-copy-service-int-test/1480463791632/mytopic-test-2016-05-17-00.gz]
 INFO async-thread-macro-1 fileape.io - create new file  :gzip   /Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/target/hdfs-copy-service-int-test/1480463791632/hdfs-metrics-2016-11-29-23.0.gz_137611410816737
 INFO async-thread-macro-1 fileape.io - creating file for  hdfs-metrics-2016-11-29-23  codec  :gzip  with out-buffer-size  32768  use-buffer  true
 INFO async-thread-macro-3 fileape.io - close and roll  #object[java.io.File 0x2ed75892 target/hdfs-copy-service-int-test/1480463791632/hdfs-metrics-2016-11-29-23.0.gz_137611410816737]  to  #object[java.io.File 0x6bd5d32c target/hdfs-copy-service-int-test/1480463791632/hdfs-metrics-2016-11-29-23.0.137611410816737.gz]
"Rolled file " #object[java.io.File 0x6bd5d32c "target/hdfs-copy-service-int-test/1480463791632/hdfs-metrics-2016-11-29-23.0.137611410816737.gz"]
 INFO main pseidon-hdfs.hdfs-copy-service - load-topic-meta!: query  select base_partition,
                        log_partition,
                        hive_url,
                        hive_table_name,
                        quarantine,
                        hive_user,
                        hive_password
                        from pseidon_logs where log='hdfs-metrics'
 INFO main pseidon-hdfs.hdfs-copy-service - copying file  #object[java.io.File 0x798ac54d target/hdfs-copy-service-int-test/1480463791632/hdfs-metrics-2016-11-29-23.0.137611410816737.gz]  via  /tmp/copying_log_pseidon_hdfs-metrics_dt=20161129_hr=2016112923_Gerrrits-MacBook-Pro.local-hdfs-metrics-2016-11-29-23.0.137611410816737.gz  to  /log/pseidon/hdfs-metrics/dt=20161129/hr=2016112923/Gerrrits-MacBook-Pro.local-hdfs-metrics-2016-11-29-23.0.137611410816737.gz
 INFO IPC Server handler 9 on 55016 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/log/pseidon/hdfs-metrics/dt=20161129/hr=2016112923	dst=null	perm=null	proto=rpc
 INFO main pseidon-hdfs.hdfs-util - creating hdfs dir  /log/pseidon/hdfs-metrics/dt=20161129/hr=2016112923
 INFO IPC Server handler 7 on 55016 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/log/pseidon/hdfs-metrics/dt=20161129/hr=2016112923	dst=null	perm=gvanvuuren:supergroup:rwxr-xr-x	proto=rpc
 INFO main pseidon-hdfs.hdfs-util - calling on-create
 INFO IPC Server handler 8 on 55016 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tmp/copying_log_pseidon_hdfs-metrics_dt=20161129_hr=2016112923_Gerrrits-MacBook-Pro.local-hdfs-metrics-2016-11-29-23.0.137611410816737.gz	dst=null	perm=null	proto=rpc
 INFO IPC Server handler 0 on 55016 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/tmp/copying_log_pseidon_hdfs-metrics_dt=20161129_hr=2016112923_Gerrrits-MacBook-Pro.local-hdfs-metrics-2016-11-29-23.0.137611410816737.gz	dst=null	perm=gvanvuuren:supergroup:rw-r--r--	proto=rpc
 INFO IPC Server handler 1 on 55016 org.apache.hadoop.hdfs.StateChange - BLOCK* allocateBlock: /tmp/copying_log_pseidon_hdfs-metrics_dt=20161129_hr=2016112923_Gerrrits-MacBook-Pro.local-hdfs-metrics-2016-11-29-23.0.137611410816737.gz. BP-1586230954-10.0.0.10-1480463798732 blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-a16ab906-4ea8-4205-9d03-d8565272a70a:NORMAL:127.0.0.1:55017|RBW]]}
 INFO DataXceiver for client DFSClient_NONMAPREDUCE_-636513846_1 at /127.0.0.1:55024 [Receiving block BP-1586230954-10.0.0.10-1480463798732:blk_1073741826_1002] org.apache.hadoop.hdfs.server.datanode.DataNode - Receiving BP-1586230954-10.0.0.10-1480463798732:blk_1073741826_1002 src: /127.0.0.1:55024 dest: /127.0.0.1:55017
 INFO PacketResponder: BP-1586230954-10.0.0.10-1480463798732:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace - src: /127.0.0.1:55024, dest: /127.0.0.1:55017, bytes: 198, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-636513846_1, offset: 0, srvID: fba48ee8-765d-49bb-a1c4-e8a8d4921872, blockid: BP-1586230954-10.0.0.10-1480463798732:blk_1073741826_1002, duration: 2580936
 INFO PacketResponder: BP-1586230954-10.0.0.10-1480463798732:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] org.apache.hadoop.hdfs.server.datanode.DataNode - PacketResponder: BP-1586230954-10.0.0.10-1480463798732:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
 INFO IPC Server handler 2 on 55016 BlockStateChange - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:55017 is added to blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-a16ab906-4ea8-4205-9d03-d8565272a70a:NORMAL:127.0.0.1:55017|RBW]]} size 0
 INFO IPC Server handler 3 on 55016 org.apache.hadoop.hdfs.StateChange - DIR* completeFile: /tmp/copying_log_pseidon_hdfs-metrics_dt=20161129_hr=2016112923_Gerrrits-MacBook-Pro.local-hdfs-metrics-2016-11-29-23.0.137611410816737.gz is closed by DFSClient_NONMAPREDUCE_-636513846_1
 INFO IPC Server handler 4 on 55016 org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit - allowed=true	ugi=gvanvuuren (auth:SIMPLE)	ip=/127.0.0.1	cmd=rename	src=/tmp/copying_log_pseidon_hdfs-metrics_dt=20161129_hr=2016112923_Gerrrits-MacBook-Pro.local-hdfs-metrics-2016-11-29-23.0.137611410816737.gz	dst=/log/pseidon/hdfs-metrics/dt=20161129/hr=2016112923/Gerrrits-MacBook-Pro.local-hdfs-metrics-2016-11-29-23.0.137611410816737.gz	perm=gvanvuuren:supergroup:rw-r--r--	proto=rpc
 INFO main pseidon-hdfs.hdfs-copy-service - Deleting file  #object[java.io.File 0x798ac54d target/hdfs-copy-service-int-test/1480463791632/hdfs-metrics-2016-11-29-23.0.137611410816737.gz]
 INFO main pseidon-hdfs.hdfs-copy-service - Could not write copy metrics due to shutdown and writers already shutdown topic hdfs-metrics
 INFO main com.zaxxer.hikari.pool.HikariPool - HikariPool-2 - Close initiated...
 INFO main com.zaxxer.hikari.pool.HikariPool - HikariPool-2 - Closed.
 INFO main org.apache.hadoop.hdfs.MiniDFSCluster - Shutting down DataNode 0
 WARN main org.apache.hadoop.hdfs.server.datanode.DirectoryScanner - DirectoryScanner: shutdown has been called
 INFO org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@222ebd21 org.apache.hadoop.hdfs.server.datanode.DataNode - Closing all peers.
 INFO VolumeScannerThread(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2) org.apache.hadoop.hdfs.server.datanode.VolumeScanner - VolumeScanner(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2, DS-a16ab906-4ea8-4205-9d03-d8565272a70a) exiting.
 INFO VolumeScannerThread(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1) org.apache.hadoop.hdfs.server.datanode.VolumeScanner - VolumeScanner(/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1, DS-b21ee017-2b19-4bc6-ad3e-da604daae8ee) exiting.
 INFO main org.mortbay.log - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
 INFO main org.apache.hadoop.ipc.Server - Stopping server on 55020
 WARN DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.datanode.DataNode - BPOfferService for Block pool BP-1586230954-10.0.0.10-1480463798732 (Datanode Uuid fba48ee8-765d-49bb-a1c4-e8a8d4921872) service to localhost/127.0.0.1:55016 interrupted
 WARN DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.datanode.DataNode - Ending block pool service for: Block pool BP-1586230954-10.0.0.10-1480463798732 (Datanode Uuid fba48ee8-765d-49bb-a1c4-e8a8d4921872) service to localhost/127.0.0.1:55016
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.datanode.DataNode - Removed Block pool BP-1586230954-10.0.0.10-1480463798732 (Datanode Uuid fba48ee8-765d-49bb-a1c4-e8a8d4921872)
 INFO DataNode: [[[DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data1/, [DISK]file:/Users/gvanvuuren/checkouts/pseidon/pseidon-hdfs/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55016 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - Removing block pool BP-1586230954-10.0.0.10-1480463798732
 INFO IPC Server Responder org.apache.hadoop.ipc.Server - Stopping IPC Server Responder
 INFO IPC Server listener on 55020 org.apache.hadoop.ipc.Server - Stopping IPC Server listener on 55020
 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter@378074e1 org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl - LazyWriter was interrupted, exiting
 INFO main org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService - Shutting down all async disk service threads
 INFO main org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService - All async disk service threads have been shut down
 INFO main org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService - Shutting down all async lazy persist service threads
 INFO main org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService - All async lazy persist service threads have been shut down
 INFO main org.apache.hadoop.hdfs.server.datanode.DataNode - Shutdown complete.

Ran 1 tests containing 1 assertions.
0 failures, 0 errors.
 INFO main com.zaxxer.hikari.HikariDataSource - HikariPool-3 - Started.
 INFO main hsqldb.db.HSQLDB58B284DE01.ENGINE - Checkpoint start
 INFO main hsqldb.db.HSQLDB58B284DE01.ENGINE - checkpointClose start
 INFO main hsqldb.db.HSQLDB58B284DE01.ENGINE - checkpointClose end
 INFO main hsqldb.db.HSQLDB58B284DE01.ENGINE - Checkpoint end - txts: 1
 INFO main com.zaxxer.hikari.pool.PoolBase - HikariPool-3 - Driver does not support get/set network timeout for connections. (feature not supported)
 INFO main com.zaxxer.hikari.HikariDataSource - HikariPool-4 - Started.
 INFO main com.zaxxer.hikari.pool.PoolBase - HikariPool-4 - Driver does not support get/set network timeout for connections. (feature not supported)
 INFO main pseidon-hdfs.hdfs-copy-service - load-topic-meta!: query  select base_partition,
                        log_partition,
                        hive_url,
                        hive_table_name,
                        quarantine,
                        hive_user,
                        hive_password
                        from pseidon_logs where log='mylog'
 INFO main pseidon-hdfs.hdfs-copy-service - load-topic-meta!: query  select base_partition,
                        log_partition,
                        hive_url,
                        hive_table_name,
                        quarantine,
                        hive_user,
                        hive_password
                        from pseidon_logs where log='nolog'
 INFO main com.zaxxer.hikari.pool.HikariPool - HikariPool-4 - Close initiated...
 INFO main com.zaxxer.hikari.pool.HikariPool - HikariPool-4 - Closed.

Ran 2 tests containing 2 assertions.
0 failures, 0 errors.
 INFO main com.zaxxer.hikari.HikariDataSource - HikariPool-5 - Started.
 INFO main hsqldb.db.HSQLDB58B284DE31.ENGINE - Checkpoint start
 INFO main hsqldb.db.HSQLDB58B284DE31.ENGINE - checkpointClose start
 INFO main hsqldb.db.HSQLDB58B284DE31.ENGINE - checkpointClose end
 INFO main hsqldb.db.HSQLDB58B284DE31.ENGINE - Checkpoint end - txts: 1
 INFO main com.zaxxer.hikari.pool.PoolBase - HikariPool-5 - Driver does not support get/set network timeout for connections. (feature not supported)
 INFO main com.zaxxer.hikari.HikariDataSource - HikariPool-6 - Started.
 INFO main com.zaxxer.hikari.pool.PoolBase - HikariPool-6 - Driver does not support get/set network timeout for connections. (feature not supported)
 INFO main com.zaxxer.hikari.pool.HikariPool - HikariPool-6 - Close initiated...
 INFO main com.zaxxer.hikari.pool.HikariPool - HikariPool-6 - Closed.

Ran 2 tests containing 1 assertions.
0 failures, 0 errors.

Ran 0 tests containing 0 assertions.
0 failures, 0 errors.
Tests run: , Assertions: 7, Failures: 2, Errors: 0
There are test failures.
 INFO Thread-175 org.apache.hive.service.server.HiveServer2 - Shutting down HiveServer2
 INFO Thread-175 org.apache.hive.service.cli.thrift.ThriftCLIService - Thrift server has stopped
 INFO Thread-175 org.apache.hive.service.AbstractService - Service:ThriftBinaryCLIService is stopped.
 INFO Thread-175 org.apache.hive.service.AbstractService - Service:OperationManager is stopped.
 INFO Thread-175 org.apache.hive.service.AbstractService - Service:SessionManager is stopped.
 INFO Thread-176 org.apache.hive.service.cli.thrift.ThriftCLIService - Started ThriftBinaryCLIService on port 10000 with 5...500 worker threads
 INFO Thread-175 org.apache.hive.service.AbstractService - Service:CLIService is stopped.
 INFO Thread-175 org.apache.hive.service.AbstractService - Service:HiveServer2 is stopped.
 INFO Thread-175 org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/static,jar:file:/Users/gvanvuuren/.m2/repository/org/apache/hive/hive-service/1.1.0-cdh5.7.0/hive-service-1.1.0-cdh5.7.0.jar!/hive-webapps/static}
 INFO Thread-175 org.eclipse.jetty.servlet.listener.ELContextCleaner - javax.el.BeanELResolver purged
 INFO Thread-175 org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.w.WebAppContext{/,file:/private/var/folders/tk/w8__4rl57830l94b5y2l7g6w0000gn/T/jetty-0.0.0.0-10002-hiveserver2-_-any-/webapp/},jar:file:/Users/gvanvuuren/.m2/repository/org/apache/hive/hive-service/1.1.0-cdh5.7.0/hive-service-1.1.0-cdh5.7.0.jar!/hive-webapps/hiveserver2
 INFO Thread-175 org.apache.hive.service.server.HiveServer2 - Web UI has stopped
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 31.449 s
[INFO] Finished at: 2016-11-30T00:56:42+01:00
[INFO] Final Memory: 25M/437M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal com.theoryinpractise:clojure-maven-plugin:1.7.1:test (test-clojure) on project pseidon-hdfs: Clojure failed. -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
